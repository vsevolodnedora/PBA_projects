{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:47:26.053032690Z",
     "start_time": "2023-12-17T10:47:25.071863755Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load prepared data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c934c8db40f0eeaf"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs=(43200, 150), pars=(43200, 7), times=(150,)\n",
      "lcs=4.1156688517299256e-13, 101.53666687011719, pars=0.0 92999999488.0, times=(150,)\n"
     ]
    }
   ],
   "source": [
    "# Read data from local\n",
    "with h5py.File(os.getcwd()+'/data/'+\"X.h5\",\"r\") as f:\n",
    "    lcs = np.array(f[\"X\"])\n",
    "    times = np.array(f[\"time\"])\n",
    "\n",
    "with h5py.File(os.getcwd()+'/data/'+\"Y.h5\",\"r\") as f:\n",
    "    pars = np.array(f[\"Y\"])\n",
    "    features_names = list(f[\"keys\"])\n",
    "\n",
    "print(f\"lcs={lcs.shape}, pars={pars.shape}, times={times.shape}\")\n",
    "print(f\"lcs={lcs.min()}, {lcs.max()}, pars={pars.min()} {pars.max()}, times={times.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:40:58.958229658Z",
     "start_time": "2023-12-17T10:40:58.927846747Z"
    }
   },
   "id": "16d04ff78df4afce"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:41:06.883466058Z",
     "start_time": "2023-12-17T10:41:05.807530751Z"
    }
   },
   "id": "394e64641b1b2361"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LightCurve dataset\n",
    "    Dispatches a lightcurve to the appropriate index\n",
    "    \"\"\"\n",
    "    def __init__(self, pars:np.ndarray, lcs:np.ndarray, times:np.ndarray, device=\"gpu\"):\n",
    "        self.device = device\n",
    "        self.pars = np.array(pars)\n",
    "        self.lcs = np.array(lcs)\n",
    "        assert self.pars.shape[0] == self.lcs.shape[0], \"size mismatch between lcs and pars\"\n",
    "        self.times = times\n",
    "        self.len = len(self.lcs)\n",
    "\n",
    "        # preprocess parameters\n",
    "        self.scaler = preprocessing.MinMaxScaler()\n",
    "        self.scaler.fit(pars)\n",
    "        self.pars_normed = self._transform_pars(pars)\n",
    "        # inverse transform\n",
    "        # inverse = scaler.inverse_transform(normalized)\n",
    "        if np.min(self.pars_normed) < 0 or np.max(self.pars_normed) > 1:\n",
    "            raise ValueError(\"Parameter normalization error\")\n",
    "        # preprocess lcs \n",
    "        self._transform_lcs(self.lcs)\n",
    "        if np.min(self.lcs_log_norm) < 0 or np.max(self.lcs_log_norm) > 1:\n",
    "            raise ValueError(\"Parameter normalization error\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" returns image/lc, vars(params)[normalized], vars(params)[physical] \"\"\"\n",
    "        return (torch.from_numpy(self.lcs_log_norm[index]).to(self.device).to(torch.float),\n",
    "                torch.from_numpy(self.pars_normed[index]).to(self.device).to(torch.float),  # .reshape(-1,1)\n",
    "                self.lcs[index],\n",
    "                self.pars[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lcs)\n",
    "\n",
    "    def _transform_lcs(self, lcs):\n",
    "        log_lcs = np.log10(lcs)\n",
    "        self.lc_min = log_lcs.min()\n",
    "        self.lc_max = log_lcs.max()\n",
    "        self.lcs_log_norm = (log_lcs - np.min(log_lcs)) / (np.max(log_lcs) - np.min(log_lcs))\n",
    "\n",
    "    def _transform_pars(self, pars):\n",
    "        return self.scaler.transform(pars)\n",
    "\n",
    "    def inverse_transform_lc_log(self, lcs_log_normed):\n",
    "        return np.power(10, lcs_log_normed * (self.lc_max - self.lc_min) + self.lc_min)\n",
    "\n",
    "    def get_dataloader(self, batch_size=32, test_split=0.2):\n",
    "        \"\"\"\n",
    "        If \n",
    "        :param batch_size: if 1 it is stochastic gradient descent, else mini-batch gradient descent\n",
    "        :param test_split: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        dataset_size = len(self)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(test_split * dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, drop_last=False)\n",
    "        test_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                 sampler=test_sampler, drop_last=False)\n",
    "\n",
    "        return (train_loader, test_loader)\n",
    "\n",
    "dataset = LightCurveDataset(pars, lcs, times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:41:30.164804845Z",
     "start_time": "2023-12-17T10:41:30.134245240Z"
    }
   },
   "id": "a1a733cc13408813"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Neural Network \n",
    "\n",
    "### Primary Sources:  \n",
    "- [Paper by Lukosiute](https://arxiv.org/pdf/2204.00285.pdf) with [GitHub code](https://github.com/klukosiute/kilonovanet) using [Bulla's data](https://github.com/mbulla/kilonova_models/tree/master/bns/bns_grids/bns_m3_3comp). \n",
    "- [PELS-VAE](https://github.com/jorgemarpa/PELS-VAE) github that had was used to draft train part for Lukosiute net ([data for it](https://zenodo.org/records/3820679#.XsW12RMzaRc))\n",
    "\n",
    "### Secondary Sources\n",
    "- [Tronto Autoencoder](https://www.cs.toronto.edu/~lczhang/aps360_20191/lec/w05/autoencoder.html) (Convolutional net)\n",
    "- [Video with derivations](https://www.youtube.com/watch?v=iwEzwTTalbg)\n",
    "- [Data sampling with scikit](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)\n",
    "- [Astro ML BOOK repo with code](https://github.com/astroML/astroML_figures/blob/742df9181f73e5c903ea0fd0894ad6af83099c96/book_figures/chapter9/fig_sdss_vae.py#L45)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a96649ad77e882"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Num of trainable params:  1862906\n",
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (layers_mu): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=700, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=700, out_features=700, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=700, out_features=28, bias=True)\n",
      "    )\n",
      "    (layers_logvar): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=700, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=700, out_features=700, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=700, out_features=28, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=35, out_features=700, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=700, out_features=700, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=700, out_features=150, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    \"\"\"\n",
    "        Base pytorch cVAE class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space (latent_units)\n",
    "        :param c: Dimension of conditioning variables\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.c = c\n",
    "        self.encoder = Encoder(image_size, hidden_dim, z_dim, c) # self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(image_size, hidden_dim, z_dim, c) # self.decoder = Decoder(latent_dims)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        \"\"\"\n",
    "        Compute one single pass through decoder and encoder\n",
    "\n",
    "        :param x: Conditioning variables corresponding to images/spectra\n",
    "        :param y: Images/spectra\n",
    "        :return: Mean returned by decoder, mean returned by encoder, log variance returned by encoder\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"1 x={} y={}\".format(x.shape, y.shape))\n",
    "        y = torch.cat((y, x), dim=1)\n",
    "        mean, logvar = self.encoder(y)\n",
    "        # print(f\"2 y={y.shape}\")\n",
    "        # re-parametrize\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + eps * std\n",
    "        # print(f\"3 sample={sample.shape}\")\n",
    "        z = torch.cat((sample, x), dim=1)\n",
    "        # print(f\"4 cat(sample,x) -> z={z.shape}\")\n",
    "        mean_dec = self.decoder(z)\n",
    "        # print(f\"5 mean_dec={mean_dec.shape}\")\n",
    "        return (mean_dec, mean, logvar, z)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "            Initialize weight of recurrent layers\n",
    "        \"\"\"\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder of the cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space\n",
    "        :param c: Dimension of conditioning variables\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.Linear(latent_dims, 512)\n",
    "        self.layers_mu = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "        self.layers_logvar = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute single pass through the encoder\n",
    "\n",
    "        :param x: Concatenated images and corresponding conditioning variables\n",
    "        :return: Mean and log variance of the encoder's distribution\n",
    "        \"\"\"\n",
    "        mean = self.layers_mu(x)\n",
    "        logvar = self.layers_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder of cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(z_dim + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, image_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Compute single pass through the decoder\n",
    "\n",
    "        :param z: Concatenated sample of hidden variables and the originally inputted conditioning variables\n",
    "        :return: Mean of decoder's distirbution\n",
    "        \"\"\"\n",
    "        mean = self.layers(z)\n",
    "        return mean\n",
    "\n",
    "\n",
    "\n",
    "model = CVAE(image_size=len(lcs[0]),  #  150\n",
    "             hidden_dim=700,\n",
    "             z_dim=len(pars[0])*4,\n",
    "             c=len(pars[0])) # TODO -------------------  check! \n",
    "print('Summary:')\n",
    "\n",
    "n_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Num of trainable params: ', n_train_params)\n",
    "print(model)   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:42:10.412781472Z",
     "start_time": "2023-12-17T10:42:10.369949986Z"
    }
   },
   "id": "cfad76a106034092"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer    : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 1e-06\n",
      "    lr: 1e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "LR Scheduler : StepLR\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize optimizers\n",
    "# lgorithms that adjust the model's parameters during training to minimize a loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.e-6) # TODO ------------ CHECK\n",
    "\n",
    "# Initialize learning Rate scheduler\n",
    "def select_scheduler(lr_sch='step')->optim.lr_scheduler or None:\n",
    "    if lr_sch == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              step_size=10,\n",
    "                                              gamma=0.1)\n",
    "    elif lr_sch == 'exp':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                     gamma=0.985)\n",
    "    elif lr_sch == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=50,\n",
    "                                                         eta_min=1e-5)\n",
    "    elif lr_sch == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=.5,\n",
    "                                                         verbose=True)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler\n",
    "scheduler = select_scheduler()\n",
    "\n",
    "print('Optimizer    :', optimizer)\n",
    "print('LR Scheduler :', scheduler.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:42:23.887234897Z",
     "start_time": "2023-12-17T10:42:23.538323794Z"
    }
   },
   "id": "6d516c7720c97487"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't\n",
    "    improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        patience  : int\n",
    "            How long to wait after last time validation loss improved.\n",
    "            Default: 7\n",
    "        min_delta : float\n",
    "            Minimum change in monitored value to qualify as \n",
    "            improvement. This number should be positive.\n",
    "            Default: 0\n",
    "        verbose   : bool\n",
    "            If True, prints a message for each validation loss improvement.\n",
    "            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        current_loss = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_loss\n",
    "        elif torch.abs(current_loss - self.best_score) < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = current_loss\n",
    "            self.counter = 0\n",
    "\n",
    "def save_checkpoint(model, optimizer, save_path, epoch):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, save_path)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model:CVAE, optimizer, batch_size, scheduler=None,\n",
    "                 beta='step', print_every=50, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1 and True:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model.to(self.device)\n",
    "        print('Is model in cuda? ', next(self.model.parameters()).is_cuda)\n",
    "        self.opt = optimizer\n",
    "        self.sch = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                           'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                           'wMSE': []}\n",
    "        self.test_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                          'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                          'wMSE': []}\n",
    "        self.num_steps = 0\n",
    "        self.print_every = print_every\n",
    "        self.beta = beta\n",
    "\n",
    "        # --- \n",
    "        self.model_dir = os.getcwd() + '/models/'\n",
    "        self.run_name = \"test\"\n",
    "\n",
    "    def train(self, train_loader, test_loader, epochs, save=True, save_chkpt=True, early_stop=False):\n",
    "        # hold samples, real and generated, for initial plotting\n",
    "        if early_stop:\n",
    "            early_stopping = EarlyStopping(patience=10, min_delta=.01, verbose=True)\n",
    "\n",
    "        # train for n number of epochs\n",
    "        time_start = datetime.datetime.now()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            e_time = datetime.datetime.now()\n",
    "            print('##'*20)\n",
    "            print(\"\\nEpoch {}\".format(epoch))\n",
    "            # train and validate\n",
    "            train_running_loss = self._train_epoch(train_loader, epoch)\n",
    "            train_running_loss /= len(train_loader)\n",
    "            train_losses.append(train_running_loss)\n",
    "\n",
    "            # test and get the final loss\n",
    "            val_loss, val_running_loss = self._test_epoch(test_loader, epoch)\n",
    "            val_running_loss /= len(test_loader)\n",
    "            val_losses.append(val_running_loss)\n",
    "\n",
    "            # update learning rate according to scheduler\n",
    "            if self.sch is not None:\n",
    "                if 'ReduceLROnPlateau' == self.sch.__class__.__name__:\n",
    "                    self.sch.step(val_loss)\n",
    "                else:\n",
    "                    self.sch.step(epoch)\n",
    "\n",
    "            # report elapsed time per epoch and total run time\n",
    "            epoch_time = datetime.datetime.now() - e_time\n",
    "            elap_time = datetime.datetime.now() - time_start\n",
    "\n",
    "            print(f\"\\tTime per epoch {epoch_time.seconds} s, Beta={self._beta_scheduler(epoch):.2f}\")\n",
    "            print(f\"\\tTrain loss {train_running_loss} Validation loss {val_running_loss}\")\n",
    "            print(f\"\\tElapsed time {elap_time.seconds/60:.2f} m\")\n",
    "            print('##'*20)\n",
    "\n",
    "            if save_chkpt:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_running_loss,\n",
    "                    'val_loss': val_running_loss,\n",
    "                    'beta':self._beta_scheduler(epoch)\n",
    "                },\n",
    "                    '%s/VAE_model_%s_%d.chkpt' % (self.model_dir, self.run_name, epoch))\n",
    "\n",
    "            # early stopping\n",
    "            if early_stop:\n",
    "                early_stopping(val_loss.cpu())\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        if save:\n",
    "            print(\"Saving model...\")\n",
    "\n",
    "            torch.save({\n",
    "                \"model_state_dict\":self.model.state_dict(),\n",
    "                \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            },\n",
    "                '%s/VAE_model_%s.pt' % (self.model_dir, self.run_name))\n",
    "\n",
    "        return (train_losses, val_losses)\n",
    "\n",
    "    def _test_epoch(self, test_loader, epoch):\n",
    "        \"\"\"Testing loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # set model in an 'eval' mode\n",
    "        self.model.eval()\n",
    "        running_loss = 0\n",
    "        with torch.no_grad():\n",
    "            xhat_plot, x_plot, l_plot = [], [], []\n",
    "            for i, (data, label, onehot, pp) in enumerate(test_loader):\n",
    "                # move data to device where model is\n",
    "                data = data.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                # evaluate model on the data\n",
    "                xhat, mu, logvar, z = self.model(data, label)\n",
    "                # compute loss \n",
    "                loss = self._loss(data, xhat, mu, logvar, train=False, ep=epoch)\n",
    "                # save batch loss\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "\n",
    "        self._report_test(epoch)\n",
    "\n",
    "        return (loss, running_loss)\n",
    "\n",
    "    def _train_epoch(self, data_loader, epoch):\n",
    "        \"\"\"Training loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "\n",
    "        # set model into training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # iterate over len(data)/batch_size\n",
    "        mu_ep, labels = [], []\n",
    "        xhat_plot, x_plot, l_plot = [], [], []\n",
    "        running_loss = 0.0\n",
    "        # Get data for [start:start+batch_size] for each epoch\n",
    "        for i, (data, label, data_phys, label_phys) in enumerate(data_loader):\n",
    "            self.num_steps += 1\n",
    "            # mode train data to device\n",
    "            data = data.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            # print(f\"data={data.shape} label={label.shape}\")\n",
    "            # Resets the gradients of all optimized tensors\n",
    "            self.opt.zero_grad()\n",
    "            # evaluate model \n",
    "            xhat, mu, logvar, z = self.model(data, label) # (mean_dec, mean, logvar, z)\n",
    "            # print(f\"x_hat={xhat.shape}, mu={mu.shape}, logvar={logvar.shape}, z={z.shape} data={data.shape} label={label.shape}\")\n",
    "            # compute loss \n",
    "            loss = self._loss(data, xhat, mu, logvar, train=True, ep=epoch)\n",
    "            # computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "            loss.backward()\n",
    "            # perform a single optimization step\n",
    "            self.opt.step()\n",
    "            # print train loss\n",
    "            self._report_train(i)\n",
    "            # save the loss\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            # TODO add logging\n",
    "        return running_loss\n",
    "    def _loss(self, x, xhat, mu, logvar, train=True, ep=0):\n",
    "        \"\"\"Evaluates loss function and add reports to the logger.\n",
    "        Loss function is weighted MSe + KL divergeance. Also BCE\n",
    "        is calculate for comparison.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x      : tensor\n",
    "            tensor of real values\n",
    "        xhat   : tensor\n",
    "            tensor of predicted values\n",
    "        mu     : tensor\n",
    "            tensor of mean values\n",
    "        logvar : tensor\n",
    "            tensor of log vairance values\n",
    "        train  : bool\n",
    "            wheather is training step or not\n",
    "        ep     : int\n",
    "            epoch value of training loop\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss\n",
    "            loss value\n",
    "        \"\"\"\n",
    "        bce = F.binary_cross_entropy(xhat, x, reduction='mean')\n",
    "        mse = F.mse_loss(xhat, x, reduction='mean')\n",
    "\n",
    "        kld_l = -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp()) / x.shape[0] #/ 1e4\n",
    "        #0 kld_o = -1. * F.kl_div(xhat, x, reduction='mean')\n",
    "        loss = mse + self._beta_scheduler(ep) * kld_l  # + 1 * kld_o\n",
    "\n",
    "        if train:\n",
    "            self.train_loss['BCE'].append(bce.item())\n",
    "            self.train_loss['MSE'].append(mse.item())\n",
    "            self.train_loss['KL_latent'].append(kld_l.item())\n",
    "            self.train_loss['Loss'].append(loss.item())\n",
    "        else:\n",
    "            self.test_loss['BCE'].append(bce.item())\n",
    "            self.test_loss['MSE'].append(mse.item())\n",
    "            self.test_loss['KL_latent'].append(kld_l.item())\n",
    "            self.test_loss['Loss'].append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def _report_train(self, i):\n",
    "        \"\"\"Report training metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        if (i % self.print_every == 0):\n",
    "            print(\"Training iteration %i, global step %i\" % (i + 1, self.num_steps))\n",
    "            print(\"BCE : %3.4f\" % (self.train_loss['BCE'][-1]))\n",
    "            print(\"MSE : %3.4f\" % (self.train_loss['MSE'][-1]))\n",
    "            print(\"KL_l: %3.4f\" % (self.train_loss['KL_latent'][-1]))\n",
    "            print(\"Loss: %3.4f\" % (self.train_loss['Loss'][-1]))\n",
    "            print(\"__\"*20)\n",
    "\n",
    "    def _report_test(self, ep):\n",
    "        \"\"\"Report testing metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        print('*** TEST LOSS ***')\n",
    "        print(\"Epoch %i, global step %i\" % (ep, self.num_steps))\n",
    "        print(\"BCE : %3.4f\" % (self.test_loss['BCE'][-1]))\n",
    "        print(\"MSE : %3.4f\" % (self.test_loss['MSE'][-1]))\n",
    "        print(\"KL_l: %3.4f\" % (self.test_loss['KL_latent'][-1]))\n",
    "        print(\"Loss: %3.4f\" % (self.test_loss['Loss'][-1]))\n",
    "\n",
    "        print(\"__\"*20)\n",
    "\n",
    "    def _beta_scheduler(self, epoch, beta0=0., step=50, gamma=0.1):\n",
    "        \"\"\"Scheduler for beta value, the sheduler is a step function that\n",
    "        increases beta value after \"step\" number of epochs by a factor \"gamma\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            epoch value\n",
    "        beta0 : float\n",
    "            starting beta value\n",
    "        step  : int\n",
    "            epoch step for update\n",
    "        gamma : float\n",
    "            linear factor of step scheduler\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta\n",
    "            beta value\n",
    "        \"\"\"\n",
    "\n",
    "        if self.beta == 'step':\n",
    "            return beta0 + gamma * (epoch // step)\n",
    "        else:\n",
    "            return float(self.beta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T10:42:45.800538801Z",
     "start_time": "2023-12-17T10:42:44.719075586Z"
    }
   },
   "id": "1932d89673f5d0e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize trainer \n",
    "trainer = Trainer(model=model, optimizer=optimizer, batch_size=64,\n",
    "                  print_every=200, scheduler=scheduler,\n",
    "                  device=device, beta=0.01)\n",
    "\n",
    "# create data loaders (feed data to model for each fold)\n",
    "train_loader, test_loader = dataset.get_dataloader(batch_size=64, test_split=.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d474bb1575393ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loss, val_loss = trainer.train(train_loader, test_loader, epochs=100, save=True, early_stop=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a4a1ce47e0db39f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Model Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a511906d980916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(train_loss), train_loss, label=\"Train\")\n",
    "plt.plot(range(val_loss), val_loss, label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf18702311336a5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "def inference(pars:list, model:CVAE, dataset:LightCurveDataset, device):\n",
    "    # if len(pars) != model.z_dim:\n",
    "    #     raise ValueError(f\"Number of parameters = {len(pars)} does not match the model latent space size {model.z_dim}\")\n",
    "    # create state vector for intput data (repeat physical parameters for times needed)\n",
    "    pars = np.asarray(pars).reshape(1, -1)\n",
    "    # normalize parameters as in the training data\n",
    "    normed_pars = dataset._transform_pars(pars=pars)\n",
    "    # generate prediction\n",
    "    with torch.no_grad():\n",
    "        # convert intput data to the format of the hidden space\n",
    "        z = (torch.zeros((1, model.z_dim)).repeat((len(normed_pars), 1)).to(device).to(torch.float))\n",
    "        # create the input for the decoder \n",
    "        decoder_input = torch.cat((z, torch.from_numpy(normed_pars).to(device).to(torch.float)), dim=1)\n",
    "        # perform reconstruction using model\n",
    "        reconstructions = model.decoder(decoder_input)\n",
    "    # move prediction to cpu and numpy\n",
    "    reconstructions_np = reconstructions.double().cpu().detach().numpy()\n",
    "    # undo normalization that was done in training data\n",
    "    lc_nn = dataset.inverse_transform_lc_log(reconstructions_np)\n",
    "    return lc_nn\n",
    "# 'eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq'\n",
    "new_pars = [.1, 0.01, 1., 2.2, 0., 0.001, 2.4e9]\n",
    "lc = inference(new_pars, model, dataset, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d068fd7a12a484"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_one(i = 0):\n",
    "\n",
    "    lc = lcs[i]\n",
    "    par = pars[i]\n",
    "    title = \"\".join([_par+\"=\"+f\"{_val:.2e} \" for _par,_val in zip(features_names,par)])\n",
    "    plt.loglog(times, lc, color='gray')\n",
    "\n",
    "    lc_nn = inference(par, model, dataset, device)\n",
    "    lc_nn = lc_nn[0]\n",
    "    plt.ylabel(\"Flux Density [mJy]\")\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.loglog(times, lc_nn, color='blue')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_one(10)\n",
    "plot_one(1000)\n",
    "plot_one(10000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda355ca8df544bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model:CVAE, dataset:LightCurveDataset, device):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fde9c17bca8a830"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
