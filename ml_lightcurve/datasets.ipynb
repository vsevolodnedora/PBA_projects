{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare train/test datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7864b2248c8090"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:02.323992268Z",
     "start_time": "2023-12-13T14:27:02.321276954Z"
    }
   },
   "id": "e060c71845ec3c51"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6480000 entries, 0 to 6479999\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   eps_e      float64\n",
      " 1   eps_b      float64\n",
      " 2   eps_t      float64\n",
      " 3   p          float64\n",
      " 4   theta_obs  float64\n",
      " 5   n_ism      float64\n",
      " 6   freq       float64\n",
      " 7   time       float64\n",
      " 8   flux       float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 494.4 MB\n",
      "None\n",
      "File loaded: /media/vsevolod/T7/work/prj_kn_afterglow/SFHoTim276_13_14_0025_150mstg_B0_HLLC/collated.csv None\n"
     ]
    }
   ],
   "source": [
    "AFGRUNDIR = \"/media/vsevolod/T7/work/prj_kn_afterglow/\"\n",
    "sim = {}; sim[\"name\"] = \"SFHoTim276_13_14_0025_150mstg_B0_HLLC\"\n",
    "collated_file_path = AFGRUNDIR + sim[\"name\"] + '/' + \"collated.csv\"\n",
    "\n",
    "assert os.path.isfile(collated_file_path), \"Collated file not found\"\n",
    "df = pd.read_csv(collated_file_path, index_col=0)\n",
    "print(f\"File loaded: {collated_file_path} {print(df.info(memory_usage='deep'))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:17.603035489Z",
     "start_time": "2023-12-13T14:27:02.628408251Z"
    }
   },
   "id": "6c8e869ea16916fc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "target = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:17.613763894Z",
     "start_time": "2023-12-13T14:27:17.603328307Z"
    }
   },
   "id": "546d36906b51b489"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t> Visualizing SFHoTim276_13_14_0025_150mstg_B0_HLLC Shape: (6480000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   eps_e  eps_b  eps_t    p  theta_obs  n_ism          freq           time  \\\n0  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  100000.000000   \n1  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  106332.657164   \n\n           flux  \n0  7.278929e-11  \n1  8.460537e-11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eps_e</th>\n      <th>eps_b</th>\n      <th>eps_t</th>\n      <th>p</th>\n      <th>theta_obs</th>\n      <th>n_ism</th>\n      <th>freq</th>\n      <th>time</th>\n      <th>flux</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>100000.000000</td>\n      <td>7.278929e-11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>106332.657164</td>\n      <td>8.460537e-11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Duplicated_rows: 0\n",
      "\t> Numeric features: 9 \n",
      "Index(['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq', 'time',\n",
      "       'flux'],\n",
      "      dtype='object')\n",
      "\t> Object features: 0 \n",
      "Index([], dtype='object')\n",
      "\t Analyzing SFHoTim276_13_14_0025_150mstg_B0_HLLC Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           is_unique   unique  with_nan  percent_nan           min  \\\neps_e          False        5     False          0.0  1.000000e-03   \neps_b          False        5     False          0.0  1.000000e-03   \neps_t          False        4     False          0.0  1.000000e-02   \np              False        4     False          0.0  2.200000e+00   \ntheta_obs      False        3     False          0.0  0.000000e+00   \nn_ism          False        6     False          0.0  1.000000e-03   \nfreq           False        6     False          0.0  2.400000e+09   \ntime           False      150     False          0.0  1.000000e+05   \nflux            True  6480000     False          0.0  4.115669e-13   \n\n                    max          mean    dtype  \neps_e      5.000000e-01  1.322000e-01  float64  \neps_b      5.000000e-01  1.322000e-01  float64  \neps_t      1.000000e+00  4.025000e-01  float64  \np          2.800000e+00  2.500000e+00  float64  \ntheta_obs  1.570796e+00  7.853982e-01  float64  \nn_ism      1.000000e+00  2.768333e-01  float64  \nfreq       9.300000e+10  3.123333e+10  float64  \ntime       9.404449e+08  1.052639e+08  float64  \nflux       1.015367e+02  1.489473e-01  float64  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_unique</th>\n      <th>unique</th>\n      <th>with_nan</th>\n      <th>percent_nan</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>dtype</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>eps_e</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_b</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_t</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-02</td>\n      <td>1.000000e+00</td>\n      <td>4.025000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>p</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.200000e+00</td>\n      <td>2.800000e+00</td>\n      <td>2.500000e+00</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>theta_obs</th>\n      <td>False</td>\n      <td>3</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>1.570796e+00</td>\n      <td>7.853982e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>n_ism</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>1.000000e+00</td>\n      <td>2.768333e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.400000e+09</td>\n      <td>9.300000e+10</td>\n      <td>3.123333e+10</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>time</th>\n      <td>False</td>\n      <td>150</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e+05</td>\n      <td>9.404449e+08</td>\n      <td>1.052639e+08</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>flux</th>\n      <td>True</td>\n      <td>6480000</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>4.115669e-13</td>\n      <td>1.015367e+02</td>\n      <td>1.489473e-01</td>\n      <td>float64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualize_df(df:pd.DataFrame, name:str):\n",
    "    print(f\"\\t> Visualizing {name} Shape: {df.shape}\")\n",
    "\n",
    "    display(df.head(2))\n",
    "\n",
    "    print(f\"\\t Duplicated_rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    # check df properties\n",
    "    def analyze_df(df : pd.DataFrame)->pd.DataFrame:\n",
    "        res = pd.DataFrame({\n",
    "            \"is_unique\": df.nunique() == len(df),\n",
    "            \"unique\": df.nunique(),\n",
    "            \"with_nan\":df.isna().any(),\n",
    "            \"percent_nan\":round((df.isnull().sum()/len(df))*100,4),\n",
    "            \"min\":df.min(),\n",
    "            \"max\":df.max(),\n",
    "            \"mean\":df.mean(),\n",
    "            \"dtype\":df.dtypes\n",
    "        })\n",
    "        return res\n",
    "    print(f\"\\t> Numeric features: {df.select_dtypes(exclude='object').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='object').keys()}\")\n",
    "    print(f\"\\t> Object features: {df.select_dtypes(exclude='number').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='number').keys()}\")\n",
    "    print(f\"\\t Analyzing {name} Summary:\")\n",
    "    metadata = analyze_df(df=df)\n",
    "    return metadata\n",
    "metadata = _visualize_df(df=df, name=sim[\"name\"])\n",
    "display(metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:23.045525412Z",
     "start_time": "2023-12-13T14:27:17.612033636Z"
    }
   },
   "id": "40829df753c3f4a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select and tansform features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c8fcf2d5bd3de73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Set target\n",
    "metadata[\"target\"] = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:23.047395745Z",
     "start_time": "2023-12-13T14:27:23.045218264Z"
    }
   },
   "id": "7bc789908ccb6eac"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of light curves: 43200 times: 150\n"
     ]
    }
   ],
   "source": [
    "# Print total number of lightcurves\n",
    "n_curves = np.prod([metadata[\"unique\"][key] for key in df.columns if key not in [\"flux\",\"time\"]])\n",
    "n_times = metadata[\"unique\"][\"time\"]\n",
    "print(f\"total number of light curves: {n_curves} times: {n_times}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:23.058623760Z",
     "start_time": "2023-12-13T14:27:23.047606380Z"
    }
   },
   "id": "68ea15e9e6e17967"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[ 1  2  3  4  5 10]\n",
      " [ 1  2  3  4  5 20]\n",
      " [ 1  2  3  4  5 30]]\n"
     ]
    }
   ],
   "source": [
    "unique_times = np.array([10, 20, 30])\n",
    "physical_parameters = np.array([1,2,3,4,5])\n",
    "all_data_input = np.hstack((\n",
    "    np.repeat(physical_parameters.reshape(1, -1), len(unique_times), axis=0),\n",
    "    unique_times.reshape(-1, 1)\n",
    "))\n",
    "print(all_data_input.shape)\n",
    "print(all_data_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:23.059497418Z",
     "start_time": "2023-12-13T14:27:23.049967079Z"
    }
   },
   "id": "7c24dde49043d444"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# features_names = [col for col in list(df.columns) if col not in [\"flux\", \"time\"]]\n",
    "# def prepare_scaling_metadata(df:pd.DataFrame, features_names:list[str]):\n",
    "#     metadata = {}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:27:23.060354032Z",
     "start_time": "2023-12-13T14:27:23.056907023Z"
    }
   },
   "id": "3e0bb411056dcca3"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target name: 'flux' features_names: ['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq']\n",
      "Total number of light curves: 43200 times: 150\n",
      "lcs=(43200, 150), pars=(43200, 7), times=(150,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data in numpy arrays\n",
    "def LcCollatedDataFrameToNumpyArray(df:pd.DataFrame, metadata:pd.DataFrame,target=\"flux\",time=\"time\"):\n",
    "\n",
    "    features_names = [col for col in list(df.columns) if col not in [target, time]]\n",
    "    print(f\"Target name: '{target}' features_names: {features_names}\")\n",
    "\n",
    "    n_curves = np.prod([metadata[\"unique\"][key] for key in features_names])#df.columns if key not in [\"flux\",\"time\"]])\n",
    "    n_times = metadata[\"unique\"][\"time\"]\n",
    "    print(f\"Total number of light curves: {n_curves} times: {n_times}\")\n",
    "\n",
    "    grouped = df.groupby(features_names)\n",
    "    pars = np.vstack([np.array(key) for key, val in grouped.groups.items()])\n",
    "    lcs  = np.vstack([np.array(df[target].iloc[val]) for key, val in grouped.groups.items()])\n",
    "    times= np.array(np.array(df[time].unique()))\n",
    "    assert len(times) == len(lcs[0])\n",
    "    assert n_curves == len(lcs)\n",
    "    return (lcs, pars, times)\n",
    "lcs, pars, times = LcCollatedDataFrameToNumpyArray(df, metadata)  \n",
    "print(f\"lcs={lcs.shape}, pars={pars.shape}, times={times.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T15:39:43.627117395Z",
     "start_time": "2023-12-13T15:39:30.712219712Z"
    }
   },
   "id": "c1c2ebdc52c257c2"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T15:22:10.542368519Z",
     "start_time": "2023-12-13T15:22:10.498905952Z"
    }
   },
   "id": "7e6217ac6b697405"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1156688662493273e-13 101.5366661484814\n",
      "-12.3855595744698 2.0066228996921094\n",
      "-3.6362396384942115 -3.5715946045498184\n",
      "0.0 1.0\n",
      "-12.3855595744698 2.0066228996921094\n",
      "4.1156688662493283e-13 101.5366661484814\n"
     ]
    }
   ],
   "source": [
    "lcs_log = np.log10(lcs)\n",
    "print(np.min(lcs), np.max(lcs))\n",
    "print(np.min(lcs_log), np.max(lcs_log))\n",
    "print(np.mean(lcs_log), np.median(lcs_log))\n",
    "\n",
    "lcs_log_norm = (lcs_log - np.min(lcs_log)) / (np.max(lcs_log) - np.min(lcs_log))\n",
    "print(np.min(lcs_log_norm), np.max(lcs_log_norm))\n",
    "\n",
    "lcs_log_rec =  (np.max(lcs_log) - np.min(lcs_log)) * lcs_log_norm + np.min(lcs_log)\n",
    "lcs_rec = np.power(10., lcs_log_rec)\n",
    "print(np.min(lcs_log_rec), np.max(lcs_log_rec))\n",
    "print(np.min(lcs_rec), np.max(lcs_rec))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:12:10.112848409Z",
     "start_time": "2023-12-13T16:12:09.957012952Z"
    }
   },
   "id": "702843071f9170e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c752457f632bad00"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, min=0.001 max=0.5\n",
      "i=1, min=0.001 max=0.5\n",
      "i=2, min=0.01 max=1.0\n",
      "i=3, min=2.2 max=2.8\n",
      "i=4, min=0.0 max=1.5707963267948966\n",
      "i=5, min=0.001 max=1.0\n",
      "i=6, min=2400000000.0 max=93000000000.0\n",
      "scaler.data_max_=[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "scaler.data_min_=[1.0e-03 1.0e-03 1.0e-02 2.2e+00 0.0e+00 1.0e-03 2.4e+09]\n",
      "scaler.data_range_=[4.99000000e-01 4.99000000e-01 9.90000000e-01 6.00000000e-01\n",
      " 1.57079633e+00 9.99000000e-01 9.06000000e+10]\n",
      "(43200, 7)\n",
      "[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# log_pars = pars.copy()\n",
    "# log_pars[:,0:4] = np.log10(pars[:,0:4])\n",
    "# log_pars[:,6:8] = np.log10(pars[:,6:8])\n",
    "\n",
    "for i in range(len(pars[0,:])):\n",
    "    print(f\"i={i}, min={np.min(pars[:,i])} max={np.max(pars[:,i])}\")\n",
    "\n",
    "# pars[6:8] = np.log10(pars[6:8])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(pars)\n",
    "normalized = scaler.transform(pars)\n",
    "print(f\"scaler.data_max_={scaler.data_max_}\")\n",
    "print(f\"scaler.data_min_={scaler.data_min_}\")\n",
    "print(f\"scaler.data_range_={scaler.data_range_}\")\n",
    "print(normalized.shape)\n",
    "print(pars[-1])\n",
    "print(normalized[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T15:28:51.294287299Z",
     "start_time": "2023-12-13T15:28:51.249593719Z"
    }
   },
   "id": "369da66db0bce58d"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "\n",
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LightCurve dataset\n",
    "    Dispatches a lightcurve to the appropriate index\n",
    "    \"\"\"\n",
    "    def __init__(self, pars:np.ndarray, lcs:np.ndarray, times:np.ndarray):\n",
    "        self.pars = np.array(pars)\n",
    "        self.lcs = np.array(lcs)\n",
    "        assert self.pars.shape[0] == self.lcs.shape[0], \"size mismatch between lcs and pars\"\n",
    "        self.times = times\n",
    "        self.len = len(self.lcs)\n",
    "        \n",
    "        # preprocess parameters\n",
    "        self.scaler = preprocessing.MinMaxScaler()\n",
    "        self.scaler.fit(pars)\n",
    "        self.pars_normed = self.scaler.transform(pars)\n",
    "        # inverse transform\n",
    "        # inverse = scaler.inverse_transform(normalized)\n",
    "        \n",
    "        # preprocess lcs \n",
    "        self._transform_lcs(self.lcs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" returns image/lc, vars(params)[normalized], vars(params)[physical] \"\"\"\n",
    "        return (self.lcs_log_norm[index], self.pars_normed[index], self.lcs[index], self.pars[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lcs)\n",
    "    \n",
    "    def _transform_lcs(self, lcs):\n",
    "        log_lcs = np.log10(lcs)\n",
    "        self.lc_min = log_lcs.min()\n",
    "        self.lc_max = log_lcs.max()\n",
    "        self.lcs_log_norm = (log_lcs - np.min(lcs_log)) / (np.max(lcs_log) - np.min(lcs_log))\n",
    "         \n",
    "    def inverse_transform_lc_log(self, lcs_log_normed):\n",
    "        return np.power(10, lcs_log_normed * (self.lc_max - self.lc_min) + self.lc_min)\n",
    "    \n",
    "    def get_dataloader(self, batch_size=32, test_split=0.2):\n",
    "        dataset_size = len(self)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(test_split * dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, drop_last=False)\n",
    "        test_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                 sampler=test_sampler, drop_last=False)\n",
    "        \n",
    "        return (train_loader, test_loader)\n",
    "dataset = LightCurveDataset(pars, lcs, times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:27:30.815498090Z",
     "start_time": "2023-12-13T16:27:30.742413337Z"
    }
   },
   "id": "3901bd960dbe089e"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from model_cvae import CVAE\n",
    "model = CVAE(image_size=len(lcs[0]), hidden_dim=200, z_dim=20, c=len(pars[0])) # TODO -------------------  check! \n",
    "train_loader, test_loader = dataset.get_dataloader(batch_size=32, test_split=.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.e-4) # TODO ------------ CHECK"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:33:24.222330686Z",
     "start_time": "2023-12-13T16:33:23.908593614Z"
    }
   },
   "id": "4181aaa665f61274"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T16:34:33.530301958Z",
     "start_time": "2023-12-13T16:34:33.486331667Z"
    }
   },
   "id": "64ffc83cd30f7413"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import datetime\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't\n",
    "    improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        patience  : int\n",
    "            How long to wait after last time validation loss improved.\n",
    "            Default: 7\n",
    "        min_delta : float\n",
    "            Minimum change in monitored value to qualify as \n",
    "            improvement. This number should be positive.\n",
    "            Default: 0\n",
    "        verbose   : bool\n",
    "            If True, prints a message for each validation loss improvement.\n",
    "            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        current_loss = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_loss\n",
    "        elif torch.abs(current_loss - self.best_score) < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = current_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "class Trainer:\n",
    "    def __init__(self, model:CVAE, optimizer, batch_size, scheduler=None, cond_l=False, cond_p=False,\n",
    "                 beta='step', print_every=50, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1 and True:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model.to(self.device)\n",
    "        print('Is model in cuda? ', next(self.model.parameters()).is_cuda)\n",
    "        self.opt = optimizer\n",
    "        self.sch = scheduler\n",
    "        self.cond_l = cond_l\n",
    "        self.cond_p = cond_p\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                           'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                           'wMSE': []}\n",
    "        self.test_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                          'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                          'wMSE': []}\n",
    "        self.num_steps = 0\n",
    "        self.print_every = print_every\n",
    "        self.beta = beta\n",
    "    \n",
    "    def train(self, train_loader, test_loader, epochs, data_ex, save=True, early_stop=False):\n",
    "        # hold samples, real and generated, for initial plotting\n",
    "        if early_stop:\n",
    "            early_stopping = EarlyStopping(patience=10, min_delta=.01, verbose=True)\n",
    "\n",
    "        # train for n number of epochs\n",
    "        time_start = datetime.datetime.now()\n",
    "\n",
    "        time_start = datetime.datetime.now()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            e_time = datetime.datetime.now()\n",
    "            print('##'*20)\n",
    "            print(\"\\nEpoch {}\".format(epoch))\n",
    "            print(\"beta: %.2f\" % self._beta_scheduler(epoch))\n",
    "\n",
    "    def _beta_scheduler(self, epoch, beta0=0., step=50, gamma=0.1):\n",
    "        \"\"\"Scheduler for beta value, the sheduler is a step function that\n",
    "        increases beta value after \"step\" number of epochs by a factor \"gamma\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            epoch value\n",
    "        beta0 : float\n",
    "            starting beta value\n",
    "        step  : int\n",
    "            epoch step for update\n",
    "        gamma : float\n",
    "            linear factor of step scheduler\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta\n",
    "            beta value\n",
    "        \"\"\"\n",
    "\n",
    "        if self.beta == 'step':\n",
    "            return beta0 + gamma * (epoch // step)\n",
    "        else:\n",
    "            return float(self.beta)\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81fd6eb67395a1ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
