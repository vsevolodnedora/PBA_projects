{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare train/test datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7864b2248c8090"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T12:35:26.454262796Z",
     "start_time": "2023-12-15T12:35:26.409353886Z"
    }
   },
   "id": "e060c71845ec3c51"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6480000 entries, 0 to 6479999\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   eps_e      float64\n",
      " 1   eps_b      float64\n",
      " 2   eps_t      float64\n",
      " 3   p          float64\n",
      " 4   theta_obs  float64\n",
      " 5   n_ism      float64\n",
      " 6   freq       float64\n",
      " 7   time       float64\n",
      " 8   flux       float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 494.4 MB\n",
      "None\n",
      "File loaded: /media/vsevolod/T7/work/prj_kn_afterglow/SFHoTim276_13_14_0025_150mstg_B0_HLLC/collated.csv None\n"
     ]
    }
   ],
   "source": [
    "AFGRUNDIR = \"/media/vsevolod/T7/work/prj_kn_afterglow/\"\n",
    "sim = {}; sim[\"name\"] = \"SFHoTim276_13_14_0025_150mstg_B0_HLLC\"\n",
    "collated_file_path = AFGRUNDIR + sim[\"name\"] + '/' + \"collated.csv\"\n",
    "\n",
    "assert os.path.isfile(collated_file_path), \"Collated file not found\"\n",
    "df = pd.read_csv(collated_file_path, index_col=0)\n",
    "print(f\"File loaded: {collated_file_path} {print(df.info(memory_usage='deep'))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:43.912944059Z",
     "start_time": "2023-12-15T10:54:39.987435608Z"
    }
   },
   "id": "6c8e869ea16916fc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "target = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:44.582992745Z",
     "start_time": "2023-12-15T10:54:44.571668146Z"
    }
   },
   "id": "546d36906b51b489"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t> Visualizing SFHoTim276_13_14_0025_150mstg_B0_HLLC Shape: (6480000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   eps_e  eps_b  eps_t    p  theta_obs  n_ism          freq           time  \\\n0  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  100000.000000   \n1  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  106332.657164   \n\n           flux  \n0  7.278929e-11  \n1  8.460537e-11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eps_e</th>\n      <th>eps_b</th>\n      <th>eps_t</th>\n      <th>p</th>\n      <th>theta_obs</th>\n      <th>n_ism</th>\n      <th>freq</th>\n      <th>time</th>\n      <th>flux</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>100000.000000</td>\n      <td>7.278929e-11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>106332.657164</td>\n      <td>8.460537e-11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Duplicated_rows: 0\n",
      "\t> Numeric features: 9 \n",
      "Index(['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq', 'time',\n",
      "       'flux'],\n",
      "      dtype='object')\n",
      "\t> Object features: 0 \n",
      "Index([], dtype='object')\n",
      "\t Analyzing SFHoTim276_13_14_0025_150mstg_B0_HLLC Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           is_unique   unique  with_nan  percent_nan           min  \\\neps_e          False        5     False          0.0  1.000000e-03   \neps_b          False        5     False          0.0  1.000000e-03   \neps_t          False        4     False          0.0  1.000000e-02   \np              False        4     False          0.0  2.200000e+00   \ntheta_obs      False        3     False          0.0  0.000000e+00   \nn_ism          False        6     False          0.0  1.000000e-03   \nfreq           False        6     False          0.0  2.400000e+09   \ntime           False      150     False          0.0  1.000000e+05   \nflux            True  6480000     False          0.0  4.115669e-13   \n\n                    max          mean    dtype  \neps_e      5.000000e-01  1.322000e-01  float64  \neps_b      5.000000e-01  1.322000e-01  float64  \neps_t      1.000000e+00  4.025000e-01  float64  \np          2.800000e+00  2.500000e+00  float64  \ntheta_obs  1.570796e+00  7.853982e-01  float64  \nn_ism      1.000000e+00  2.768333e-01  float64  \nfreq       9.300000e+10  3.123333e+10  float64  \ntime       9.404449e+08  1.052639e+08  float64  \nflux       1.015367e+02  1.489473e-01  float64  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_unique</th>\n      <th>unique</th>\n      <th>with_nan</th>\n      <th>percent_nan</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>dtype</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>eps_e</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_b</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_t</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-02</td>\n      <td>1.000000e+00</td>\n      <td>4.025000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>p</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.200000e+00</td>\n      <td>2.800000e+00</td>\n      <td>2.500000e+00</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>theta_obs</th>\n      <td>False</td>\n      <td>3</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>1.570796e+00</td>\n      <td>7.853982e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>n_ism</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>1.000000e+00</td>\n      <td>2.768333e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.400000e+09</td>\n      <td>9.300000e+10</td>\n      <td>3.123333e+10</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>time</th>\n      <td>False</td>\n      <td>150</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e+05</td>\n      <td>9.404449e+08</td>\n      <td>1.052639e+08</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>flux</th>\n      <td>True</td>\n      <td>6480000</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>4.115669e-13</td>\n      <td>1.015367e+02</td>\n      <td>1.489473e-01</td>\n      <td>float64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualize_df(df:pd.DataFrame, name:str):\n",
    "    print(f\"\\t> Visualizing {name} Shape: {df.shape}\")\n",
    "\n",
    "    display(df.head(2))\n",
    "\n",
    "    print(f\"\\t Duplicated_rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    # check df properties\n",
    "    def analyze_df(df : pd.DataFrame)->pd.DataFrame:\n",
    "        res = pd.DataFrame({\n",
    "            \"is_unique\": df.nunique() == len(df),\n",
    "            \"unique\": df.nunique(),\n",
    "            \"with_nan\":df.isna().any(),\n",
    "            \"percent_nan\":round((df.isnull().sum()/len(df))*100,4),\n",
    "            \"min\":df.min(),\n",
    "            \"max\":df.max(),\n",
    "            \"mean\":df.mean(),\n",
    "            \"dtype\":df.dtypes\n",
    "        })\n",
    "        return res\n",
    "    print(f\"\\t> Numeric features: {df.select_dtypes(exclude='object').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='object').keys()}\")\n",
    "    print(f\"\\t> Object features: {df.select_dtypes(exclude='number').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='number').keys()}\")\n",
    "    print(f\"\\t Analyzing {name} Summary:\")\n",
    "    metadata = analyze_df(df=df)\n",
    "    return metadata\n",
    "metadata = _visualize_df(df=df, name=sim[\"name\"])\n",
    "display(metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:50.538464076Z",
     "start_time": "2023-12-15T10:54:44.939280137Z"
    }
   },
   "id": "40829df753c3f4a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select and tansform features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c8fcf2d5bd3de73"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Set target\n",
    "metadata[\"target\"] = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:50.540780418Z",
     "start_time": "2023-12-15T10:54:50.539299483Z"
    }
   },
   "id": "7bc789908ccb6eac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of light curves: 43200 times: 150\n"
     ]
    }
   ],
   "source": [
    "# Print total number of lightcurves\n",
    "n_curves = np.prod([metadata[\"unique\"][key] for key in df.columns if key not in [\"flux\",\"time\"]])\n",
    "n_times = metadata[\"unique\"][\"time\"]\n",
    "print(f\"total number of light curves: {n_curves} times: {n_times}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:51.701048910Z",
     "start_time": "2023-12-15T10:54:51.697304228Z"
    }
   },
   "id": "68ea15e9e6e17967"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[ 1  2  3  4  5 10]\n",
      " [ 1  2  3  4  5 20]\n",
      " [ 1  2  3  4  5 30]]\n"
     ]
    }
   ],
   "source": [
    "unique_times = np.array([10, 20, 30])\n",
    "physical_parameters = np.array([1,2,3,4,5])\n",
    "all_data_input = np.hstack((\n",
    "    np.repeat(physical_parameters.reshape(1, -1), len(unique_times), axis=0),\n",
    "    unique_times.reshape(-1, 1)\n",
    "))\n",
    "print(all_data_input.shape)\n",
    "print(all_data_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:54:52.425039306Z",
     "start_time": "2023-12-15T10:54:52.419343415Z"
    }
   },
   "id": "7c24dde49043d444"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Numpy Arrays from Dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336bba3df8fa67fd"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target name: 'flux' features_names: ['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq']\n",
      "Total number of light curves: 43200 times: 150\n",
      "lcs=(43200, 150), pars=(43200, 7), times=(150,)\n",
      "Data saved as .h5 files\n"
     ]
    }
   ],
   "source": [
    "# Prepare data in numpy arrays\n",
    "features_names = [col for col in list(df.columns) if col not in [target, \"time\"]]\n",
    "print(f\"Target name: '{target}' features_names: {features_names}\")\n",
    "\n",
    "def LcCollatedDataFrameToNumpyArray(df:pd.DataFrame, metadata:pd.DataFrame,target=\"flux\",time=\"time\"):\n",
    "\n",
    "    n_curves = np.prod([metadata[\"unique\"][key] for key in features_names])#df.columns if key not in [\"flux\",\"time\"]])\n",
    "    n_times = metadata[\"unique\"][\"time\"]\n",
    "    print(f\"Total number of light curves: {n_curves} times: {n_times}\")\n",
    "\n",
    "    grouped = df.groupby(features_names)\n",
    "    pars = np.vstack([np.array(key) for key, val in grouped.groups.items()])\n",
    "    lcs  = np.vstack([np.array(df[target].iloc[val]) for key, val in grouped.groups.items()])\n",
    "    times= np.array(np.array(df[time].unique()))\n",
    "    assert len(times) == len(lcs[0])\n",
    "    assert n_curves == len(lcs)\n",
    "    return (lcs, pars, times)\n",
    "\n",
    "lcs, pars, times = LcCollatedDataFrameToNumpyArray(df, metadata)  \n",
    "\n",
    "print(f\"lcs={lcs.shape}, pars={pars.shape}, times={times.shape}\")\n",
    "\n",
    "with h5py.File(os.getcwd()+'/data/'+\"X.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"X\", data=lcs, dtype=np.float32)\n",
    "    f.create_dataset(\"time\", data=times, dtype=np.float64)\n",
    "\n",
    "with h5py.File(os.getcwd()+'/data/'+\"Y.h5\",\"w\") as f:\n",
    "    f.create_dataset(\"Y\", data=pars, dtype=np.float32)\n",
    "    f.create_dataset(\"keys\", data=np.array(features_names,dtype=\"S\"))\n",
    "print(\"Data saved as .h5 files\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T12:43:37.409266747Z",
     "start_time": "2023-12-15T12:43:21.482877451Z"
    }
   },
   "id": "c1c2ebdc52c257c2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:55:10.424262299Z",
     "start_time": "2023-12-15T10:55:09.482229046Z"
    }
   },
   "id": "7e6217ac6b697405"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, min=0.001 max=0.5\n",
      "i=1, min=0.001 max=0.5\n",
      "i=2, min=0.01 max=1.0\n",
      "i=3, min=2.2 max=2.8\n",
      "i=4, min=0.0 max=1.5707963267948966\n",
      "i=5, min=0.001 max=1.0\n",
      "i=6, min=2400000000.0 max=93000000000.0\n",
      "scaler.data_max_=[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "scaler.data_min_=[1.0e-03 1.0e-03 1.0e-02 2.2e+00 0.0e+00 1.0e-03 2.4e+09]\n",
      "scaler.data_range_=[4.99000000e-01 4.99000000e-01 9.90000000e-01 6.00000000e-01\n",
      " 1.57079633e+00 9.99000000e-01 9.06000000e+10]\n",
      "(43200, 7)\n",
      "[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# log_pars = pars.copy()\n",
    "# log_pars[:,0:4] = np.log10(pars[:,0:4])\n",
    "# log_pars[:,6:8] = np.log10(pars[:,6:8])\n",
    "\n",
    "for i in range(len(pars[0,:])):\n",
    "    print(f\"i={i}, min={np.min(pars[:,i])} max={np.max(pars[:,i])}\")\n",
    "\n",
    "# pars[6:8] = np.log10(pars[6:8])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(pars)\n",
    "normalized = scaler.transform(pars)\n",
    "print(f\"scaler.data_max_={scaler.data_max_}\")\n",
    "print(f\"scaler.data_min_={scaler.data_min_}\")\n",
    "print(f\"scaler.data_range_={scaler.data_range_}\")\n",
    "print(normalized.shape)\n",
    "print(pars[-1])\n",
    "print(normalized[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:55:10.628440450Z",
     "start_time": "2023-12-15T10:55:10.581310128Z"
    }
   },
   "id": "369da66db0bce58d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create DataLoader class for Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1770da8435f7486"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LightCurve dataset\n",
    "    Dispatches a lightcurve to the appropriate index\n",
    "    \"\"\"\n",
    "    def __init__(self, pars:np.ndarray, lcs:np.ndarray, times:np.ndarray):\n",
    "        self.pars = np.array(pars)\n",
    "        self.lcs = np.array(lcs)\n",
    "        assert self.pars.shape[0] == self.lcs.shape[0], \"size mismatch between lcs and pars\"\n",
    "        self.times = times\n",
    "        self.len = len(self.lcs)\n",
    "        \n",
    "        # preprocess parameters\n",
    "        self.scaler = preprocessing.MinMaxScaler()\n",
    "        self.scaler.fit(pars)\n",
    "        self.pars_normed = self._transform_pars(pars)\n",
    "        # inverse transform\n",
    "        # inverse = scaler.inverse_transform(normalized)\n",
    "        \n",
    "        # preprocess lcs \n",
    "        self._transform_lcs(self.lcs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" returns image/lc, vars(params)[normalized], vars(params)[physical] \"\"\"\n",
    "        return (torch.from_numpy(self.lcs_log_norm[index]).to('cuda').to(torch.float),\n",
    "                torch.from_numpy(self.pars_normed[index]).to('cuda').to(torch.float),  # .reshape(-1,1)\n",
    "                self.lcs[index], \n",
    "                self.pars[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lcs)\n",
    "    \n",
    "    def _transform_lcs(self, lcs):\n",
    "        log_lcs = np.log10(lcs)\n",
    "        self.lc_min = log_lcs.min()\n",
    "        self.lc_max = log_lcs.max()\n",
    "        self.lcs_log_norm = (log_lcs - np.min(lcs_log)) / (np.max(lcs_log) - np.min(lcs_log))\n",
    "    \n",
    "    def _transform_pars(self, pars):\n",
    "        return self.scaler.transform(pars)\n",
    "    \n",
    "    def inverse_transform_lc_log(self, lcs_log_normed):\n",
    "        return np.power(10, lcs_log_normed * (self.lc_max - self.lc_min) + self.lc_min)\n",
    "    \n",
    "    def get_dataloader(self, batch_size=32, test_split=0.2):\n",
    "        dataset_size = len(self)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(test_split * dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, drop_last=False)\n",
    "        test_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                 sampler=test_sampler, drop_last=False)\n",
    "        \n",
    "        return (train_loader, test_loader)\n",
    "    \n",
    "dataset = LightCurveDataset(pars, lcs, times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T10:55:10.694920217Z",
     "start_time": "2023-12-15T10:55:10.622137748Z"
    }
   },
   "id": "3901bd960dbe089e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Neural Network "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b155963f73b9e5"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Num of trainable params:  18448\n",
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (layers_mu): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=30, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=30, out_features=14, bias=True)\n",
      "    )\n",
      "    (layers_logvar): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=30, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=30, out_features=14, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=21, out_features=30, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=30, out_features=150, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    \"\"\"\n",
    "        Base pytorch cVAE class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space (latent_units)\n",
    "        :param c: Dimension of conditioning variables\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.c = c\n",
    "        self.encoder = Encoder(image_size, hidden_dim, z_dim, c) # self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(image_size, hidden_dim, z_dim, c) # self.decoder = Decoder(latent_dims)\n",
    "        # self.init_weights()\n",
    "        \n",
    "    def forward(self, y, x):\n",
    "        \"\"\"\n",
    "        Compute one single pass through decoder and encoder\n",
    "\n",
    "        :param x: Conditioning variables corresponding to images/spectra\n",
    "        :param y: Images/spectra\n",
    "        :return: Mean returned by decoder, mean returned by encoder, log variance returned by encoder\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"1 x={} y={}\".format(x.shape, y.shape))\n",
    "        y = torch.cat((y, x), dim=1)\n",
    "        mean, logvar = self.encoder(y)\n",
    "        # print(f\"2 y={y.shape}\")\n",
    "        # re-parametrize\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + eps * std\n",
    "        # print(f\"3 sample={sample.shape}\")\n",
    "        z = torch.cat((sample, x), dim=1)\n",
    "        # print(f\"4 cat(sample,x) -> z={z.shape}\")\n",
    "        mean_dec = self.decoder(z)\n",
    "        # print(f\"5 mean_dec={mean_dec.shape}\")\n",
    "        return (mean_dec, mean, logvar, z)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "            Initialize weight of recurrent layers\n",
    "        \"\"\"\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder of the cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space\n",
    "        :param c: Dimension of conditioning variables\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.Linear(latent_dims, 512)\n",
    "        self.layers_mu = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "        self.layers_logvar = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute single pass through the encoder\n",
    "\n",
    "        :param x: Concatenated images and corresponding conditioning variables\n",
    "        :return: Mean and log variance of the encoder's distribution\n",
    "        \"\"\"\n",
    "        mean = self.layers_mu(x)\n",
    "        logvar = self.layers_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder of cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(z_dim + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, image_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Compute single pass through the decoder\n",
    "\n",
    "        :param z: Concatenated sample of hidden variables and the originally inputted conditioning variables\n",
    "        :return: Mean of decoder's distirbution\n",
    "        \"\"\"\n",
    "        mean = self.layers(z)\n",
    "        return mean\n",
    "\n",
    "\n",
    "\n",
    "model = CVAE(image_size=len(lcs[0]),  #  150\n",
    "             hidden_dim=30, \n",
    "             z_dim=len(pars[0])*2, \n",
    "             c=len(pars[0])) # TODO -------------------  check! \n",
    "print('Summary:')\n",
    "\n",
    "n_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Num of trainable params: ', n_train_params)\n",
    "print(model)    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:02:19.072655544Z",
     "start_time": "2023-12-15T13:02:19.013734528Z"
    }
   },
   "id": "edfe6a07fd067f09"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer    : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "LR Scheduler : StepLR\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.e-3) # TODO ------------ CHECK\n",
    "\n",
    "# Initialize learning Rate scheduler\n",
    "def select_scheduler(lr_sch='step')->optim.lr_scheduler or None:\n",
    "    if lr_sch == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              step_size=20,\n",
    "                                              gamma=0.1)\n",
    "    elif lr_sch == 'exp':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                     gamma=0.985)\n",
    "    elif lr_sch == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=50,\n",
    "                                                         eta_min=1e-5)\n",
    "    elif lr_sch == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=.5,\n",
    "                                                         verbose=True)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler\n",
    "scheduler = select_scheduler()\n",
    "\n",
    "print('Optimizer    :', optimizer)\n",
    "print('LR Scheduler :', scheduler.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:05:26.911235139Z",
     "start_time": "2023-12-15T13:05:26.862266003Z"
    }
   },
   "id": "c42c60398e7a4823"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't\n",
    "    improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        patience  : int\n",
    "            How long to wait after last time validation loss improved.\n",
    "            Default: 7\n",
    "        min_delta : float\n",
    "            Minimum change in monitored value to qualify as \n",
    "            improvement. This number should be positive.\n",
    "            Default: 0\n",
    "        verbose   : bool\n",
    "            If True, prints a message for each validation loss improvement.\n",
    "            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        current_loss = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_loss\n",
    "        elif torch.abs(current_loss - self.best_score) < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = current_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "class Trainer:\n",
    "    def __init__(self, model:CVAE, optimizer, batch_size, scheduler=None, \n",
    "                 beta='step', print_every=50, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1 and True:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model.to(self.device)\n",
    "        print('Is model in cuda? ', next(self.model.parameters()).is_cuda)\n",
    "        self.opt = optimizer\n",
    "        self.sch = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                           'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                           'wMSE': []}\n",
    "        self.test_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                          'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                          'wMSE': []}\n",
    "        self.num_steps = 0\n",
    "        self.print_every = print_every\n",
    "        self.beta = beta\n",
    "        \n",
    "        # --- \n",
    "        self.model_dir = os.getcwd() + '/models/'\n",
    "        self.run_name = \"test\"\n",
    "    \n",
    "    def train(self, train_loader, test_loader, epochs, save=True, early_stop=False):\n",
    "        # hold samples, real and generated, for initial plotting\n",
    "        if early_stop:\n",
    "            early_stopping = EarlyStopping(patience=10, min_delta=.01, verbose=True)\n",
    "\n",
    "        # train for n number of epochs\n",
    "        time_start = datetime.datetime.now()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            e_time = datetime.datetime.now()\n",
    "            print('##'*20)\n",
    "            print(\"\\nEpoch {}\".format(epoch))\n",
    "            print(\"beta: %.2f\" % self._beta_scheduler(epoch))\n",
    "\n",
    "            # train and validate\n",
    "            self._train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # test and get the final loss\n",
    "            val_loss = self._test_epoch(test_loader, epoch)\n",
    "\n",
    "            # update learning rate according to scheduler\n",
    "            if self.sch is not None:\n",
    "                if 'ReduceLROnPlateau' == self.sch.__class__.__name__:\n",
    "                    self.sch.step(val_loss)\n",
    "                else:\n",
    "                    self.sch.step(epoch)\n",
    "\n",
    "            # report elapsed time per epoch and total run time\n",
    "            epoch_time = datetime.datetime.now() - e_time\n",
    "            elap_time = datetime.datetime.now() - time_start\n",
    "            print('Time per epoch: ', epoch_time.seconds, ' s')\n",
    "            print('Elapsed time  : %.2f m' % (elap_time.seconds/60))\n",
    "            print('##'*20)\n",
    "\n",
    "            # early stopping\n",
    "            if early_stop:\n",
    "                early_stopping(val_loss.cpu())\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if save:\n",
    "            torch.save(self.model.state_dict(),\n",
    "                       '%s/VAE_model_%s.pt' %\n",
    "                       (self.model_dir, self.run_name))\n",
    "\n",
    "    def _test_epoch(self, test_loader, epoch):\n",
    "        \"\"\"Testing loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model in an 'eval' mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            xhat_plot, x_plot, l_plot = [], [], []\n",
    "            for i, (data, label, onehot, pp) in enumerate(test_loader):\n",
    "                # move data to device where model is\n",
    "                data = data.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                # evaluate model on the data\n",
    "                xhat, mu, logvar, z = self.model(data, label)\n",
    "                # compute loss \n",
    "                loss = self._loss(data, xhat, mu, logvar, train=False, ep=epoch)\n",
    "        \n",
    "        self._report_test(epoch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _train_epoch(self, data_loader, epoch):\n",
    "        \"\"\"Training loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model into training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # iterate over len(data)/batch_size\n",
    "        mu_ep, labels = [], []\n",
    "        xhat_plot, x_plot, l_plot = [], [], []\n",
    "        for i, (data, label, data_phys, label_phys) in enumerate(data_loader):\n",
    "            self.num_steps += 1\n",
    "            # mode train data to device\n",
    "            data = data.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            # print(f\"data={data.shape} label={label.shape}\")\n",
    "            # Resets the gradients of all optimized tensors\n",
    "            self.opt.zero_grad()\n",
    "            # evaluate model \n",
    "            xhat, mu, logvar, z = self.model(data, label) # (mean_dec, mean, logvar, z)\n",
    "            # print(f\"x_hat={xhat.shape}, mu={mu.shape}, logvar={logvar.shape}, z={z.shape} data={data.shape} label={label.shape}\")\n",
    "            # compute loss \n",
    "            loss = self._loss(data, xhat, mu, logvar, train=True, ep=epoch)\n",
    "            # computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "            loss.backward()\n",
    "            # perform a single optimization step\n",
    "            self.opt.step()\n",
    "            # print train loss\n",
    "            self._report_train(i)\n",
    "            # TODO add logging\n",
    "\n",
    "    def _loss(self, x, xhat, mu, logvar, train=True, ep=0):\n",
    "        \"\"\"Evaluates loss function and add reports to the logger.\n",
    "        Loss function is weighted MSe + KL divergeance. Also BCE\n",
    "        is calculate for comparison.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x      : tensor\n",
    "            tensor of real values\n",
    "        xhat   : tensor\n",
    "            tensor of predicted values\n",
    "        mu     : tensor\n",
    "            tensor of mean values\n",
    "        logvar : tensor\n",
    "            tensor of log vairance values\n",
    "        train  : bool\n",
    "            wheather is training step or not\n",
    "        ep     : int\n",
    "            epoch value of training loop\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss\n",
    "            loss value\n",
    "        \"\"\"\n",
    "        bce = F.binary_cross_entropy(xhat, x, reduction='mean')\n",
    "        mse = F.mse_loss(xhat, x, reduction='mean')\n",
    "\n",
    "        kld_l = -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp()) #/ x.shape[0] / 1e4\n",
    "        # kld_o = -1. * F.kl_div(xhat, x, reduction='mean')\n",
    "        loss = bce + self._beta_scheduler(ep) * kld_l  # + 1 * kld_o\n",
    "\n",
    "        if train:\n",
    "            self.train_loss['BCE'].append(bce.item())\n",
    "            self.train_loss['MSE'].append(mse.item())\n",
    "            self.train_loss['KL_latent'].append(kld_l.item())\n",
    "            self.train_loss['Loss'].append(loss.item())\n",
    "        else:\n",
    "            self.test_loss['BCE'].append(bce.item())\n",
    "            self.test_loss['MSE'].append(mse.item())\n",
    "            self.test_loss['KL_latent'].append(kld_l.item())\n",
    "            self.test_loss['Loss'].append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def _report_train(self, i):\n",
    "        \"\"\"Report training metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        if (i % self.print_every == 0):\n",
    "            print(\"Training iteration %i, global step %i\" % (i + 1, self.num_steps))\n",
    "            print(\"BCE : %3.4f\" % (self.train_loss['BCE'][-1]))\n",
    "            print(\"MSE : %3.4f\" % (self.train_loss['MSE'][-1]))\n",
    "            print(\"KL_l: %3.4f\" % (self.train_loss['KL_latent'][-1]))\n",
    "            print(\"Loss: %3.4f\" % (self.train_loss['Loss'][-1]))\n",
    "            print(\"__\"*20)\n",
    "\n",
    "    def _report_test(self, ep):\n",
    "        \"\"\"Report testing metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        print('*** TEST LOSS ***')\n",
    "        print(\"Epoch %i, global step %i\" % (ep, self.num_steps))\n",
    "        print(\"BCE : %3.4f\" % (self.test_loss['BCE'][-1]))\n",
    "        print(\"MSE : %3.4f\" % (self.test_loss['MSE'][-1]))\n",
    "        print(\"KL_l: %3.4f\" % (self.test_loss['KL_latent'][-1]))\n",
    "        print(\"Loss: %3.4f\" % (self.test_loss['Loss'][-1]))\n",
    "\n",
    "        print(\"__\"*20)\n",
    "\n",
    "    def _beta_scheduler(self, epoch, beta0=0., step=50, gamma=0.1):\n",
    "        \"\"\"Scheduler for beta value, the sheduler is a step function that\n",
    "        increases beta value after \"step\" number of epochs by a factor \"gamma\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            epoch value\n",
    "        beta0 : float\n",
    "            starting beta value\n",
    "        step  : int\n",
    "            epoch step for update\n",
    "        gamma : float\n",
    "            linear factor of step scheduler\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta\n",
    "            beta value\n",
    "        \"\"\"\n",
    "\n",
    "        if self.beta == 'step':\n",
    "            return beta0 + gamma * (epoch // step)\n",
    "        else:\n",
    "            return float(self.beta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:05:27.093249706Z",
     "start_time": "2023-12-15T13:05:27.075482195Z"
    }
   },
   "id": "81fd6eb67395a1ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33dbc1318a8abcbc"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is model in cuda?  True\n"
     ]
    }
   ],
   "source": [
    "# initialize trainer \n",
    "trainer = Trainer(model=model, optimizer=optimizer, batch_size=32, \n",
    "                  print_every=200, scheduler=scheduler,\n",
    "                  device=device, beta=0.1)\n",
    "\n",
    "# create data loaders (feed data to model for each fold)\n",
    "train_loader, test_loader = dataset.get_dataloader(batch_size=32, test_split=.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:05:27.684542233Z",
     "start_time": "2023-12-15T13:05:27.676149852Z"
    }
   },
   "id": "c7fcec91f1f39d9a"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "\n",
      "Epoch 1\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 1\n",
      "BCE : 0.6329\n",
      "MSE : 0.0062\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6329\n",
      "________________________________________\n",
      "Training iteration 201, global step 201\n",
      "BCE : 0.6396\n",
      "MSE : 0.0037\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6396\n",
      "________________________________________\n",
      "Training iteration 401, global step 401\n",
      "BCE : 0.6277\n",
      "MSE : 0.0057\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6278\n",
      "________________________________________\n",
      "Training iteration 601, global step 601\n",
      "BCE : 0.6481\n",
      "MSE : 0.0063\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6481\n",
      "________________________________________\n",
      "Training iteration 801, global step 801\n",
      "BCE : 0.6524\n",
      "MSE : 0.0040\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6524\n",
      "________________________________________\n",
      "Training iteration 1001, global step 1001\n",
      "BCE : 0.6360\n",
      "MSE : 0.0077\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6361\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 1, global step 1080\n",
      "BCE : 0.6371\n",
      "MSE : 0.0051\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6371\n",
      "________________________________________\n",
      "Time per epoch:  6  s\n",
      "Elapsed time  : 0.10 m\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "Epoch 2\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 1081\n",
      "BCE : 0.6373\n",
      "MSE : 0.0051\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6373\n",
      "________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 201, global step 1281\n",
      "BCE : 0.6294\n",
      "MSE : 0.0045\n",
      "KL_l: 0.0008\n",
      "Loss: 0.6295\n",
      "________________________________________\n",
      "Training iteration 401, global step 1481\n",
      "BCE : 0.6559\n",
      "MSE : 0.0070\n",
      "KL_l: 0.0013\n",
      "Loss: 0.6561\n",
      "________________________________________\n",
      "Training iteration 601, global step 1681\n",
      "BCE : 0.6253\n",
      "MSE : 0.0040\n",
      "KL_l: 0.0007\n",
      "Loss: 0.6254\n",
      "________________________________________\n",
      "Training iteration 801, global step 1881\n",
      "BCE : 0.6340\n",
      "MSE : 0.0045\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6340\n",
      "________________________________________\n",
      "Training iteration 1001, global step 2081\n",
      "BCE : 0.6078\n",
      "MSE : 0.0042\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6078\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 2, global step 2160\n",
      "BCE : 0.6419\n",
      "MSE : 0.0036\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6420\n",
      "________________________________________\n",
      "Time per epoch:  6  s\n",
      "Elapsed time  : 0.20 m\n",
      "########################################\n",
      "EarlyStopping counter: 1 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 3\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 2161\n",
      "BCE : 0.6280\n",
      "MSE : 0.0042\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6280\n",
      "________________________________________\n",
      "Training iteration 201, global step 2361\n",
      "BCE : 0.6348\n",
      "MSE : 0.0043\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6349\n",
      "________________________________________\n",
      "Training iteration 401, global step 2561\n",
      "BCE : 0.6448\n",
      "MSE : 0.0041\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6448\n",
      "________________________________________\n",
      "Training iteration 601, global step 2761\n",
      "BCE : 0.6431\n",
      "MSE : 0.0037\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6431\n",
      "________________________________________\n",
      "Training iteration 801, global step 2961\n",
      "BCE : 0.6346\n",
      "MSE : 0.0027\n",
      "KL_l: 0.0005\n",
      "Loss: 0.6346\n",
      "________________________________________\n",
      "Training iteration 1001, global step 3161\n",
      "BCE : 0.6453\n",
      "MSE : 0.0032\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6454\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 3, global step 3240\n",
      "BCE : 0.6307\n",
      "MSE : 0.0023\n",
      "KL_l: 0.0006\n",
      "Loss: 0.6307\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.30 m\n",
      "########################################\n",
      "EarlyStopping counter: 2 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 4\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 3241\n",
      "BCE : 0.6225\n",
      "MSE : 0.0026\n",
      "KL_l: 0.0006\n",
      "Loss: 0.6226\n",
      "________________________________________\n",
      "Training iteration 201, global step 3441\n",
      "BCE : 0.6356\n",
      "MSE : 0.0023\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6356\n",
      "________________________________________\n",
      "Training iteration 401, global step 3641\n",
      "BCE : 0.6356\n",
      "MSE : 0.0013\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6356\n",
      "________________________________________\n",
      "Training iteration 601, global step 3841\n",
      "BCE : 0.6449\n",
      "MSE : 0.0018\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6449\n",
      "________________________________________\n",
      "Training iteration 801, global step 4041\n",
      "BCE : 0.6172\n",
      "MSE : 0.0014\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6173\n",
      "________________________________________\n",
      "Training iteration 1001, global step 4241\n",
      "BCE : 0.6409\n",
      "MSE : 0.0013\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6409\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 4, global step 4320\n",
      "BCE : 0.6524\n",
      "MSE : 0.0016\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6524\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.40 m\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "Epoch 5\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 4321\n",
      "BCE : 0.6277\n",
      "MSE : 0.0014\n",
      "KL_l: 0.0005\n",
      "Loss: 0.6277\n",
      "________________________________________\n",
      "Training iteration 201, global step 4521\n",
      "BCE : 0.6280\n",
      "MSE : 0.0014\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6280\n",
      "________________________________________\n",
      "Training iteration 401, global step 4721\n",
      "BCE : 0.6151\n",
      "MSE : 0.0015\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6151\n",
      "________________________________________\n",
      "Training iteration 601, global step 4921\n",
      "BCE : 0.6207\n",
      "MSE : 0.0015\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6207\n",
      "________________________________________\n",
      "Training iteration 801, global step 5121\n",
      "BCE : 0.6394\n",
      "MSE : 0.0022\n",
      "KL_l: 0.0003\n",
      "Loss: 0.6394\n",
      "________________________________________\n",
      "Training iteration 1001, global step 5321\n",
      "BCE : 0.6174\n",
      "MSE : 0.0013\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6174\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 5, global step 5400\n",
      "BCE : 0.6314\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6314\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.50 m\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "Epoch 6\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 5401\n",
      "BCE : 0.6414\n",
      "MSE : 0.0018\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6415\n",
      "________________________________________\n",
      "Training iteration 201, global step 5601\n",
      "BCE : 0.6373\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6373\n",
      "________________________________________\n",
      "Training iteration 401, global step 5801\n",
      "BCE : 0.6146\n",
      "MSE : 0.0012\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6147\n",
      "________________________________________\n",
      "Training iteration 601, global step 6001\n",
      "BCE : 0.6196\n",
      "MSE : 0.0015\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6197\n",
      "________________________________________\n",
      "Training iteration 801, global step 6201\n",
      "BCE : 0.6255\n",
      "MSE : 0.0009\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6255\n",
      "________________________________________\n",
      "Training iteration 1001, global step 6401\n",
      "BCE : 0.6310\n",
      "MSE : 0.0012\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6310\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 6, global step 6480\n",
      "BCE : 0.6298\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6299\n",
      "________________________________________\n",
      "Time per epoch:  6  s\n",
      "Elapsed time  : 0.60 m\n",
      "########################################\n",
      "EarlyStopping counter: 1 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 7\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 6481\n",
      "BCE : 0.6314\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6314\n",
      "________________________________________\n",
      "Training iteration 201, global step 6681\n",
      "BCE : 0.6201\n",
      "MSE : 0.0007\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6201\n",
      "________________________________________\n",
      "Training iteration 401, global step 6881\n",
      "BCE : 0.6383\n",
      "MSE : 0.0011\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6383\n",
      "________________________________________\n",
      "Training iteration 601, global step 7081\n",
      "BCE : 0.6428\n",
      "MSE : 0.0012\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6428\n",
      "________________________________________\n",
      "Training iteration 801, global step 7281\n",
      "BCE : 0.6296\n",
      "MSE : 0.0014\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6296\n",
      "________________________________________\n",
      "Training iteration 1001, global step 7481\n",
      "BCE : 0.6390\n",
      "MSE : 0.0011\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6391\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 7, global step 7560\n",
      "BCE : 0.6413\n",
      "MSE : 0.0009\n",
      "KL_l: 0.0004\n",
      "Loss: 0.6414\n",
      "________________________________________\n",
      "Time per epoch:  6  s\n",
      "Elapsed time  : 0.70 m\n",
      "########################################\n",
      "EarlyStopping counter: 2 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 8\n",
      "beta: 0.10\n",
      "Training iteration 1, global step 7561\n",
      "BCE : 0.6176\n",
      "MSE : 0.0013\n",
      "KL_l: 0.0005\n",
      "Loss: 0.6176\n",
      "________________________________________\n",
      "Training iteration 201, global step 7761\n",
      "BCE : 0.6261\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0002\n",
      "Loss: 0.6261\n",
      "________________________________________\n",
      "Training iteration 401, global step 7961\n",
      "BCE : 0.6170\n",
      "MSE : 0.0008\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6170\n",
      "________________________________________\n",
      "Training iteration 601, global step 8161\n",
      "BCE : 0.6356\n",
      "MSE : 0.0008\n",
      "KL_l: 0.0001\n",
      "Loss: 0.6356\n",
      "________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain(train_loader, test_loader, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m400\u001B[39m, save\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, early_stop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[97], line 93\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_loader, test_loader, epochs, save, early_stop)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta: \u001B[39m\u001B[38;5;132;01m%.2f\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_beta_scheduler(epoch))\n\u001B[1;32m     92\u001B[0m \u001B[38;5;66;03m# train and validate\u001B[39;00m\n\u001B[0;32m---> 93\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_epoch(train_loader, epoch)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# test and get the final loss\u001B[39;00m\n\u001B[1;32m     96\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_test_epoch(test_loader, epoch)\n",
      "Cell \u001B[0;32mIn[97], line 180\u001B[0m, in \u001B[0;36mTrainer._train_epoch\u001B[0;34m(self, data_loader, epoch)\u001B[0m\n\u001B[1;32m    178\u001B[0m mu_ep, labels \u001B[38;5;241m=\u001B[39m [], []\n\u001B[1;32m    179\u001B[0m xhat_plot, x_plot, l_plot \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[0;32m--> 180\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (data, label, data_phys, label_phys) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;66;03m# mode train data to device\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[12], line 25\u001B[0m, in \u001B[0;36mLightCurveDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[1;32m     24\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" returns image/lc, vars(params)[normalized], vars(params)[physical] \"\"\"\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlcs_log_norm[index])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat),\n\u001B[1;32m     26\u001B[0m             torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpars_normed[index])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat),  \u001B[38;5;66;03m# .reshape(-1,1)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlcs[index], \n\u001B[1;32m     28\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpars[index])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, test_loader, epochs=400, save=True, early_stop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:06:13.842137489Z",
     "start_time": "2023-12-15T13:05:28.069497567Z"
    }
   },
   "id": "ad1bdfcbb1b0f37f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyze Performance of the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25dc5ac742c49fd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create predictor\n",
    "def inference(pars:list, model:CVAE, dataset:LightCurveDataset, device):\n",
    "    # if len(pars) != model.z_dim:\n",
    "    #     raise ValueError(f\"Number of parameters = {len(pars)} does not match the model latent space size {model.z_dim}\")\n",
    "    # create state vector for intput data (repeat physical parameters for times needed)\n",
    "    pars = np.asarray(pars).reshape(1, -1)\n",
    "    # normalize parameters as in the training data\n",
    "    normed_pars = dataset._transform_pars(pars=pars)\n",
    "    # generate prediction\n",
    "    with torch.no_grad():\n",
    "        # convert intput data to the format of the hidden space\n",
    "        z = (torch.zeros((1, model.z_dim)).repeat((len(normed_pars), 1)).to(device).to(torch.float))\n",
    "        # create the input for the decoder \n",
    "        decoder_input = torch.cat((z, torch.from_numpy(normed_pars).to(device).to(torch.float)), dim=1)\n",
    "        # perform reconstruction using model\n",
    "        reconstructions = model.decoder(decoder_input)\n",
    "    # move prediction to cpu and numpy\n",
    "    reconstructions_np = reconstructions.double().cpu().detach().numpy()\n",
    "    # undo normalization that was done in training data\n",
    "    lc_nn = dataset.inverse_transform_lc_log(reconstructions_np)\n",
    "    return lc_nn\n",
    "# 'eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq'\n",
    "new_pars = [.1, 0.01, 1., 2.2, 0., 0.01, 2.4e9]\n",
    "lc = inference(new_pars, model, dataset, device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-15T13:02:04.125402538Z"
    }
   },
   "id": "656945f3c1968637"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.loglog(dataset.times, lc[0])\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:02:04.127216653Z",
     "start_time": "2023-12-15T13:02:04.126595195Z"
    }
   },
   "id": "3abd0d9d3f64b87f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21e165c310138070"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
