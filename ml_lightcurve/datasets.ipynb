{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare train/test datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7864b2248c8090"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:49:54.621046236Z",
     "start_time": "2023-12-14T14:49:54.616730775Z"
    }
   },
   "id": "e060c71845ec3c51"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6480000 entries, 0 to 6479999\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   eps_e      float64\n",
      " 1   eps_b      float64\n",
      " 2   eps_t      float64\n",
      " 3   p          float64\n",
      " 4   theta_obs  float64\n",
      " 5   n_ism      float64\n",
      " 6   freq       float64\n",
      " 7   time       float64\n",
      " 8   flux       float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 494.4 MB\n",
      "None\n",
      "File loaded: /media/vsevolod/T7/work/prj_kn_afterglow/SFHoTim276_13_14_0025_150mstg_B0_HLLC/collated.csv None\n"
     ]
    }
   ],
   "source": [
    "AFGRUNDIR = \"/media/vsevolod/T7/work/prj_kn_afterglow/\"\n",
    "sim = {}; sim[\"name\"] = \"SFHoTim276_13_14_0025_150mstg_B0_HLLC\"\n",
    "collated_file_path = AFGRUNDIR + sim[\"name\"] + '/' + \"collated.csv\"\n",
    "\n",
    "assert os.path.isfile(collated_file_path), \"Collated file not found\"\n",
    "df = pd.read_csv(collated_file_path, index_col=0)\n",
    "print(f\"File loaded: {collated_file_path} {print(df.info(memory_usage='deep'))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:49:59.315335286Z",
     "start_time": "2023-12-14T14:49:55.344282041Z"
    }
   },
   "id": "6c8e869ea16916fc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "target = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:49:59.317377955Z",
     "start_time": "2023-12-14T14:49:59.315534568Z"
    }
   },
   "id": "546d36906b51b489"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t> Visualizing SFHoTim276_13_14_0025_150mstg_B0_HLLC Shape: (6480000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   eps_e  eps_b  eps_t    p  theta_obs  n_ism          freq           time  \\\n0  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  100000.000000   \n1  0.001  0.001   0.01  2.2        0.0  0.001  2.400000e+09  106332.657164   \n\n           flux  \n0  7.278929e-11  \n1  8.460537e-11  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eps_e</th>\n      <th>eps_b</th>\n      <th>eps_t</th>\n      <th>p</th>\n      <th>theta_obs</th>\n      <th>n_ism</th>\n      <th>freq</th>\n      <th>time</th>\n      <th>flux</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>100000.000000</td>\n      <td>7.278929e-11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001</td>\n      <td>0.001</td>\n      <td>0.01</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>2.400000e+09</td>\n      <td>106332.657164</td>\n      <td>8.460537e-11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Duplicated_rows: 0\n",
      "\t> Numeric features: 9 \n",
      "Index(['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq', 'time',\n",
      "       'flux'],\n",
      "      dtype='object')\n",
      "\t> Object features: 0 \n",
      "Index([], dtype='object')\n",
      "\t Analyzing SFHoTim276_13_14_0025_150mstg_B0_HLLC Summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": "           is_unique   unique  with_nan  percent_nan           min  \\\neps_e          False        5     False          0.0  1.000000e-03   \neps_b          False        5     False          0.0  1.000000e-03   \neps_t          False        4     False          0.0  1.000000e-02   \np              False        4     False          0.0  2.200000e+00   \ntheta_obs      False        3     False          0.0  0.000000e+00   \nn_ism          False        6     False          0.0  1.000000e-03   \nfreq           False        6     False          0.0  2.400000e+09   \ntime           False      150     False          0.0  1.000000e+05   \nflux            True  6480000     False          0.0  4.115669e-13   \n\n                    max          mean    dtype  \neps_e      5.000000e-01  1.322000e-01  float64  \neps_b      5.000000e-01  1.322000e-01  float64  \neps_t      1.000000e+00  4.025000e-01  float64  \np          2.800000e+00  2.500000e+00  float64  \ntheta_obs  1.570796e+00  7.853982e-01  float64  \nn_ism      1.000000e+00  2.768333e-01  float64  \nfreq       9.300000e+10  3.123333e+10  float64  \ntime       9.404449e+08  1.052639e+08  float64  \nflux       1.015367e+02  1.489473e-01  float64  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_unique</th>\n      <th>unique</th>\n      <th>with_nan</th>\n      <th>percent_nan</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>dtype</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>eps_e</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_b</th>\n      <td>False</td>\n      <td>5</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>5.000000e-01</td>\n      <td>1.322000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>eps_t</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-02</td>\n      <td>1.000000e+00</td>\n      <td>4.025000e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>p</th>\n      <td>False</td>\n      <td>4</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.200000e+00</td>\n      <td>2.800000e+00</td>\n      <td>2.500000e+00</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>theta_obs</th>\n      <td>False</td>\n      <td>3</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.000000e+00</td>\n      <td>1.570796e+00</td>\n      <td>7.853982e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>n_ism</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e-03</td>\n      <td>1.000000e+00</td>\n      <td>2.768333e-01</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>False</td>\n      <td>6</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>2.400000e+09</td>\n      <td>9.300000e+10</td>\n      <td>3.123333e+10</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>time</th>\n      <td>False</td>\n      <td>150</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.000000e+05</td>\n      <td>9.404449e+08</td>\n      <td>1.052639e+08</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>flux</th>\n      <td>True</td>\n      <td>6480000</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>4.115669e-13</td>\n      <td>1.015367e+02</td>\n      <td>1.489473e-01</td>\n      <td>float64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _visualize_df(df:pd.DataFrame, name:str):\n",
    "    print(f\"\\t> Visualizing {name} Shape: {df.shape}\")\n",
    "\n",
    "    display(df.head(2))\n",
    "\n",
    "    print(f\"\\t Duplicated_rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    # check df properties\n",
    "    def analyze_df(df : pd.DataFrame)->pd.DataFrame:\n",
    "        res = pd.DataFrame({\n",
    "            \"is_unique\": df.nunique() == len(df),\n",
    "            \"unique\": df.nunique(),\n",
    "            \"with_nan\":df.isna().any(),\n",
    "            \"percent_nan\":round((df.isnull().sum()/len(df))*100,4),\n",
    "            \"min\":df.min(),\n",
    "            \"max\":df.max(),\n",
    "            \"mean\":df.mean(),\n",
    "            \"dtype\":df.dtypes\n",
    "        })\n",
    "        return res\n",
    "    print(f\"\\t> Numeric features: {df.select_dtypes(exclude='object').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='object').keys()}\")\n",
    "    print(f\"\\t> Object features: {df.select_dtypes(exclude='number').shape[1]} \\n\"\n",
    "          f\"{df.select_dtypes(exclude='number').keys()}\")\n",
    "    print(f\"\\t Analyzing {name} Summary:\")\n",
    "    metadata = analyze_df(df=df)\n",
    "    return metadata\n",
    "metadata = _visualize_df(df=df, name=sim[\"name\"])\n",
    "display(metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:05.255683857Z",
     "start_time": "2023-12-14T14:49:59.318503282Z"
    }
   },
   "id": "40829df753c3f4a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select and tansform features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c8fcf2d5bd3de73"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Set target\n",
    "metadata[\"target\"] = \"flux\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:05.258506796Z",
     "start_time": "2023-12-14T14:50:05.255840875Z"
    }
   },
   "id": "7bc789908ccb6eac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of light curves: 43200 times: 150\n"
     ]
    }
   ],
   "source": [
    "# Print total number of lightcurves\n",
    "n_curves = np.prod([metadata[\"unique\"][key] for key in df.columns if key not in [\"flux\",\"time\"]])\n",
    "n_times = metadata[\"unique\"][\"time\"]\n",
    "print(f\"total number of light curves: {n_curves} times: {n_times}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:05.260303513Z",
     "start_time": "2023-12-14T14:50:05.257619025Z"
    }
   },
   "id": "68ea15e9e6e17967"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[ 1  2  3  4  5 10]\n",
      " [ 1  2  3  4  5 20]\n",
      " [ 1  2  3  4  5 30]]\n"
     ]
    }
   ],
   "source": [
    "unique_times = np.array([10, 20, 30])\n",
    "physical_parameters = np.array([1,2,3,4,5])\n",
    "all_data_input = np.hstack((\n",
    "    np.repeat(physical_parameters.reshape(1, -1), len(unique_times), axis=0),\n",
    "    unique_times.reshape(-1, 1)\n",
    "))\n",
    "print(all_data_input.shape)\n",
    "print(all_data_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:05.326428028Z",
     "start_time": "2023-12-14T14:50:05.261751530Z"
    }
   },
   "id": "7c24dde49043d444"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# features_names = [col for col in list(df.columns) if col not in [\"flux\", \"time\"]]\n",
    "# def prepare_scaling_metadata(df:pd.DataFrame, features_names:list[str]):\n",
    "#     metadata = {}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:05.327225397Z",
     "start_time": "2023-12-14T14:50:05.305890782Z"
    }
   },
   "id": "3e0bb411056dcca3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target name: 'flux' features_names: ['eps_e', 'eps_b', 'eps_t', 'p', 'theta_obs', 'n_ism', 'freq']\n",
      "Total number of light curves: 43200 times: 150\n",
      "lcs=(43200, 150), pars=(43200, 7), times=(150,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data in numpy arrays\n",
    "def LcCollatedDataFrameToNumpyArray(df:pd.DataFrame, metadata:pd.DataFrame,target=\"flux\",time=\"time\"):\n",
    "\n",
    "    features_names = [col for col in list(df.columns) if col not in [target, time]]\n",
    "    print(f\"Target name: '{target}' features_names: {features_names}\")\n",
    "\n",
    "    n_curves = np.prod([metadata[\"unique\"][key] for key in features_names])#df.columns if key not in [\"flux\",\"time\"]])\n",
    "    n_times = metadata[\"unique\"][\"time\"]\n",
    "    print(f\"Total number of light curves: {n_curves} times: {n_times}\")\n",
    "\n",
    "    grouped = df.groupby(features_names)\n",
    "    pars = np.vstack([np.array(key) for key, val in grouped.groups.items()])\n",
    "    lcs  = np.vstack([np.array(df[target].iloc[val]) for key, val in grouped.groups.items()])\n",
    "    times= np.array(np.array(df[time].unique()))\n",
    "    assert len(times) == len(lcs[0])\n",
    "    assert n_curves == len(lcs)\n",
    "    return (lcs, pars, times)\n",
    "lcs, pars, times = LcCollatedDataFrameToNumpyArray(df, metadata)  \n",
    "print(f\"lcs={lcs.shape}, pars={pars.shape}, times={times.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:20.338221954Z",
     "start_time": "2023-12-14T14:50:07.470762435Z"
    }
   },
   "id": "c1c2ebdc52c257c2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:21.318973985Z",
     "start_time": "2023-12-14T14:50:20.337980538Z"
    }
   },
   "id": "7e6217ac6b697405"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1156688662493273e-13 101.5366661484814\n",
      "-12.3855595744698 2.0066228996921094\n",
      "-3.6362396384942115 -3.5715946045498184\n",
      "0.0 1.0\n",
      "-12.3855595744698 2.0066228996921094\n",
      "4.1156688662493283e-13 101.5366661484814\n"
     ]
    }
   ],
   "source": [
    "lcs_log = np.log10(lcs)\n",
    "print(np.min(lcs), np.max(lcs))\n",
    "print(np.min(lcs_log), np.max(lcs_log))\n",
    "print(np.mean(lcs_log), np.median(lcs_log))\n",
    "\n",
    "lcs_log_norm = (lcs_log - np.min(lcs_log)) / (np.max(lcs_log) - np.min(lcs_log))\n",
    "print(np.min(lcs_log_norm), np.max(lcs_log_norm))\n",
    "\n",
    "lcs_log_rec =  (np.max(lcs_log) - np.min(lcs_log)) * lcs_log_norm + np.min(lcs_log)\n",
    "lcs_rec = np.power(10., lcs_log_rec)\n",
    "print(np.min(lcs_log_rec), np.max(lcs_log_rec))\n",
    "print(np.min(lcs_rec), np.max(lcs_rec))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:21.477482727Z",
     "start_time": "2023-12-14T14:50:21.321163986Z"
    }
   },
   "id": "702843071f9170e7"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, min=0.001 max=0.5\n",
      "i=1, min=0.001 max=0.5\n",
      "i=2, min=0.01 max=1.0\n",
      "i=3, min=2.2 max=2.8\n",
      "i=4, min=0.0 max=1.5707963267948966\n",
      "i=5, min=0.001 max=1.0\n",
      "i=6, min=2400000000.0 max=93000000000.0\n",
      "scaler.data_max_=[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "scaler.data_min_=[1.0e-03 1.0e-03 1.0e-02 2.2e+00 0.0e+00 1.0e-03 2.4e+09]\n",
      "scaler.data_range_=[4.99000000e-01 4.99000000e-01 9.90000000e-01 6.00000000e-01\n",
      " 1.57079633e+00 9.99000000e-01 9.06000000e+10]\n",
      "(43200, 7)\n",
      "[5.00000000e-01 5.00000000e-01 1.00000000e+00 2.80000000e+00\n",
      " 1.57079633e+00 1.00000000e+00 9.30000000e+10]\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# log_pars = pars.copy()\n",
    "# log_pars[:,0:4] = np.log10(pars[:,0:4])\n",
    "# log_pars[:,6:8] = np.log10(pars[:,6:8])\n",
    "\n",
    "for i in range(len(pars[0,:])):\n",
    "    print(f\"i={i}, min={np.min(pars[:,i])} max={np.max(pars[:,i])}\")\n",
    "\n",
    "# pars[6:8] = np.log10(pars[6:8])\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(pars)\n",
    "normalized = scaler.transform(pars)\n",
    "print(f\"scaler.data_max_={scaler.data_max_}\")\n",
    "print(f\"scaler.data_min_={scaler.data_min_}\")\n",
    "print(f\"scaler.data_range_={scaler.data_range_}\")\n",
    "print(normalized.shape)\n",
    "print(pars[-1])\n",
    "print(normalized[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:50:21.522110808Z",
     "start_time": "2023-12-14T14:50:21.518084945Z"
    }
   },
   "id": "369da66db0bce58d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LightCurve dataset\n",
    "    Dispatches a lightcurve to the appropriate index\n",
    "    \"\"\"\n",
    "    def __init__(self, pars:np.ndarray, lcs:np.ndarray, times:np.ndarray):\n",
    "        self.pars = np.array(pars)\n",
    "        self.lcs = np.array(lcs)\n",
    "        assert self.pars.shape[0] == self.lcs.shape[0], \"size mismatch between lcs and pars\"\n",
    "        self.times = times\n",
    "        self.len = len(self.lcs)\n",
    "        \n",
    "        # preprocess parameters\n",
    "        self.scaler = preprocessing.MinMaxScaler()\n",
    "        self.scaler.fit(pars)\n",
    "        self.pars_normed = self.scaler.transform(pars)\n",
    "        # inverse transform\n",
    "        # inverse = scaler.inverse_transform(normalized)\n",
    "        \n",
    "        # preprocess lcs \n",
    "        self._transform_lcs(self.lcs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" returns image/lc, vars(params)[normalized], vars(params)[physical] \"\"\"\n",
    "        return (torch.from_numpy(self.lcs_log_norm[index]).to('cuda').to(torch.float),\n",
    "                torch.from_numpy(self.pars_normed[index]).to('cuda').to(torch.float),  # .reshape(-1,1)\n",
    "                self.lcs[index], \n",
    "                self.pars[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lcs)\n",
    "    \n",
    "    def _transform_lcs(self, lcs):\n",
    "        log_lcs = np.log10(lcs)\n",
    "        self.lc_min = log_lcs.min()\n",
    "        self.lc_max = log_lcs.max()\n",
    "        self.lcs_log_norm = (log_lcs - np.min(lcs_log)) / (np.max(lcs_log) - np.min(lcs_log))\n",
    "         \n",
    "    def inverse_transform_lc_log(self, lcs_log_normed):\n",
    "        return np.power(10, lcs_log_normed * (self.lc_max - self.lc_min) + self.lc_min)\n",
    "    \n",
    "    def get_dataloader(self, batch_size=32, test_split=0.2):\n",
    "        dataset_size = len(self)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(test_split * dataset_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating PT data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                  sampler=train_sampler, drop_last=False)\n",
    "        test_loader = DataLoader(self, batch_size=batch_size,\n",
    "                                 sampler=test_sampler, drop_last=False)\n",
    "        \n",
    "        return (train_loader, test_loader)\n",
    "    \n",
    "dataset = LightCurveDataset(pars, lcs, times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T14:51:35.124427518Z",
     "start_time": "2023-12-14T14:51:35.065831145Z"
    }
   },
   "id": "3901bd960dbe089e"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Num of trainable params:  33020\n",
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (layers_mu): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
      "    )\n",
      "    (layers_logvar): Sequential(\n",
      "      (0): Linear(in_features=157, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=50, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=17, out_features=50, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "      (3): Tanh()\n",
      "      (4): Linear(in_features=50, out_features=150, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    \"\"\"\n",
    "        Base pytorch cVAE class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space\n",
    "        :param c: Dimension of conditioning variables\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder = Encoder(image_size, hidden_dim, z_dim, c) # self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(image_size, hidden_dim, z_dim, c) # self.decoder = Decoder(latent_dims)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, y, x):\n",
    "        \"\"\"\n",
    "        Compute one single pass through decoder and encoder\n",
    "\n",
    "        :param x: Conditioning variables corresponding to images/spectra\n",
    "        :param y: Images/spectra\n",
    "        :return: Mean returned by decoder, mean returned by encoder, log variance returned by encoder\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"1 x={} y={}\".format(x.shape, y.shape))\n",
    "        y = torch.cat((y, x), dim=1)\n",
    "        mean, logvar = self.encoder(y)\n",
    "        # print(f\"2 y={y.shape}\")\n",
    "        # re-parametrize\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + eps * std\n",
    "        # print(f\"3 sample={sample.shape}\")\n",
    "        z = torch.cat((sample, x), dim=1)\n",
    "        # print(f\"4 cat(sample,x) -> z={z.shape}\")\n",
    "        mean_dec = self.decoder(z)\n",
    "        # print(f\"5 mean_dec={mean_dec.shape}\")\n",
    "        return (mean_dec, mean, logvar, z)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "            Initialize weight of recurrent layers\n",
    "        \"\"\"\n",
    "        for name, param in self.encoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.normal_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder of the cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        \"\"\"\n",
    "        :param image_size: Size of 1D \"images\" of data set i.e. spectrum size\n",
    "        :param hidden_dim: Dimension of hidden layer\n",
    "        :param z_dim: Dimension of latent space\n",
    "        :param c: Dimension of conditioning variables\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # nn.Linear(latent_dims, 512)\n",
    "        self.layers_mu = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "        self.layers_logvar = nn.Sequential(\n",
    "            nn.Linear(image_size + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute single pass through the encoder\n",
    "\n",
    "        :param x: Concatenated images and corresponding conditioning variables\n",
    "        :return: Mean and log variance of the encoder's distribution\n",
    "        \"\"\"\n",
    "        mean = self.layers_mu(x)\n",
    "        logvar = self.layers_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder of cVAE\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=150, hidden_dim=50, z_dim=10, c=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(z_dim + c, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, image_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Compute single pass through the decoder\n",
    "\n",
    "        :param z: Concatenated sample of hidden variables and the originally inputted conditioning variables\n",
    "        :return: Mean of decoder's distirbution\n",
    "        \"\"\"\n",
    "        mean = self.layers(z)\n",
    "        return mean\n",
    "\n",
    "\n",
    "\n",
    "model = CVAE(image_size=len(lcs[0]),  #  150\n",
    "             hidden_dim=50, \n",
    "             z_dim=10, \n",
    "             c=len(pars[0])) # TODO -------------------  check! \n",
    "print('Summary:')\n",
    "\n",
    "n_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Num of trainable params: ', n_train_params)\n",
    "print(model)    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:27:30.066519862Z",
     "start_time": "2023-12-14T15:27:30.025578190Z"
    }
   },
   "id": "edfe6a07fd067f09"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer    : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.0001\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "LR Scheduler : StepLR\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.e-4) # TODO ------------ CHECK\n",
    "\n",
    "# Initialize learning Rate scheduler\n",
    "def select_scheduler(lr_sch=\"step\")->optim.lr_scheduler or None:\n",
    "    if lr_sch == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              step_size=20,\n",
    "                                              gamma=0.5)\n",
    "    elif lr_sch == 'exp':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                     gamma=0.985)\n",
    "    elif lr_sch == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                         T_max=50,\n",
    "                                                         eta_min=1e-5)\n",
    "    elif lr_sch == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=.5,\n",
    "                                                         verbose=True)\n",
    "    else:\n",
    "        scheduler = None\n",
    "    return scheduler\n",
    "scheduler = select_scheduler()\n",
    "\n",
    "print('Optimizer    :', optimizer)\n",
    "print('LR Scheduler :', scheduler.__class__.__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:27:30.426537285Z",
     "start_time": "2023-12-14T15:27:30.396868676Z"
    }
   },
   "id": "c42c60398e7a4823"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't\n",
    "    improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        patience  : int\n",
    "            How long to wait after last time validation loss improved.\n",
    "            Default: 7\n",
    "        min_delta : float\n",
    "            Minimum change in monitored value to qualify as \n",
    "            improvement. This number should be positive.\n",
    "            Default: 0\n",
    "        verbose   : bool\n",
    "            If True, prints a message for each validation loss improvement.\n",
    "            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.min_delta = min_delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        current_loss = val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_loss\n",
    "        elif torch.abs(current_loss - self.best_score) < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} / {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = current_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "class Trainer:\n",
    "    def __init__(self, model:CVAE, optimizer, batch_size, scheduler=None, \n",
    "                 beta='step', print_every=50, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1 and True:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.model.to(self.device)\n",
    "        print('Is model in cuda? ', next(self.model.parameters()).is_cuda)\n",
    "        self.opt = optimizer\n",
    "        self.sch = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                           'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                           'wMSE': []}\n",
    "        self.test_loss = {'KL_latent': [], 'BCE': [], 'Loss': [],\n",
    "                          'MSE': [], 'KL_output': [], 'tMSE': [],\n",
    "                          'wMSE': []}\n",
    "        self.num_steps = 0\n",
    "        self.print_every = print_every\n",
    "        self.beta = beta\n",
    "        \n",
    "        # --- \n",
    "        self.model_dir = os.getcwd() + '/models/'\n",
    "        self.run_name = \"test\"\n",
    "    \n",
    "    def train(self, train_loader, test_loader, epochs, save=True, early_stop=False):\n",
    "        # hold samples, real and generated, for initial plotting\n",
    "        if early_stop:\n",
    "            early_stopping = EarlyStopping(patience=10, min_delta=.01, verbose=True)\n",
    "\n",
    "        # train for n number of epochs\n",
    "        time_start = datetime.datetime.now()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            e_time = datetime.datetime.now()\n",
    "            print('##'*20)\n",
    "            print(\"\\nEpoch {}\".format(epoch))\n",
    "            print(\"beta: %.2f\" % self._beta_scheduler(epoch))\n",
    "\n",
    "            # train and validate\n",
    "            self._train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # test and get the final loss\n",
    "            val_loss = self._test_epoch(test_loader, epoch)\n",
    "\n",
    "            # update learning rate according to scheduler\n",
    "            if self.sch is not None:\n",
    "                if 'ReduceLROnPlateau' == self.sch.__class__.__name__:\n",
    "                    self.sch.step(val_loss)\n",
    "                else:\n",
    "                    self.sch.step(epoch)\n",
    "\n",
    "            # report elapsed time per epoch and total run time\n",
    "            epoch_time = datetime.datetime.now() - e_time\n",
    "            elap_time = datetime.datetime.now() - time_start\n",
    "            print('Time per epoch: ', epoch_time.seconds, ' s')\n",
    "            print('Elapsed time  : %.2f m' % (elap_time.seconds/60))\n",
    "            print('##'*20)\n",
    "\n",
    "            # early stopping\n",
    "            if early_stop:\n",
    "                early_stopping(val_loss.cpu())\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        if save:\n",
    "            torch.save(self.model.state_dict(),\n",
    "                       '%s/VAE_model_%s.pt' %\n",
    "                       (self.model_dir, self.run_name))\n",
    "\n",
    "    def _test_epoch(self, test_loader, epoch):\n",
    "        \"\"\"Testing loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model in an 'eval' mode\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            xhat_plot, x_plot, l_plot = [], [], []\n",
    "            for i, (data, label, onehot, pp) in enumerate(test_loader):\n",
    "                # move data to device where model is\n",
    "                data = data.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                # evaluate model on the data\n",
    "                xhat, mu, logvar, z = self.model(data, label)\n",
    "                # compute loss \n",
    "                loss = self._loss(data, xhat, mu, logvar, train=False, ep=epoch)\n",
    "        \n",
    "        self._report_test(epoch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _train_epoch(self, data_loader, epoch):\n",
    "        \"\"\"Training loop for a given epoch. Triningo goes over\n",
    "        batches, light curves and latent space plots are logged to\n",
    "        W&B logger\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader : pytorch object\n",
    "            data loader object with training items\n",
    "        epoch       : int\n",
    "            epoch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        \n",
    "        # set model into training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # iterate over len(data)/batch_size\n",
    "        mu_ep, labels = [], []\n",
    "        xhat_plot, x_plot, l_plot = [], [], []\n",
    "        for i, (data, label, data_phys, label_phys) in enumerate(data_loader):\n",
    "            self.num_steps += 1\n",
    "            # mode train data to device\n",
    "            data = data.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            # print(f\"data={data.shape} label={label.shape}\")\n",
    "            # Resets the gradients of all optimized tensors\n",
    "            self.opt.zero_grad()\n",
    "            # evaluate model \n",
    "            xhat, mu, logvar, z = self.model(data, label) # (mean_dec, mean, logvar, z)\n",
    "            # print(f\"x_hat={xhat.shape}, mu={mu.shape}, logvar={logvar.shape}, z={z.shape} data={data.shape} label={label.shape}\")\n",
    "            # compute loss \n",
    "            loss = self._loss(data, xhat, mu, logvar, train=True, ep=epoch)\n",
    "            # computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "            loss.backward()\n",
    "            # perform a single optimization step\n",
    "            self.opt.step()\n",
    "            # print train loss\n",
    "            self._report_train(i)\n",
    "            # TODO add logging\n",
    "\n",
    "    def _loss(self, x, xhat, mu, logvar, train=True, ep=0):\n",
    "        \"\"\"Evaluates loss function and add reports to the logger.\n",
    "        Loss function is weighted MSe + KL divergeance. Also BCE\n",
    "        is calculate for comparison.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x      : tensor\n",
    "            tensor of real values\n",
    "        xhat   : tensor\n",
    "            tensor of predicted values\n",
    "        mu     : tensor\n",
    "            tensor of mean values\n",
    "        logvar : tensor\n",
    "            tensor of log vairance values\n",
    "        train  : bool\n",
    "            wheather is training step or not\n",
    "        ep     : int\n",
    "            epoch value of training loop\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss\n",
    "            loss value\n",
    "        \"\"\"\n",
    "        bce = F.binary_cross_entropy(xhat, x, reduction='mean')\n",
    "        mse = F.mse_loss(xhat, x, reduction='mean')\n",
    "\n",
    "        kld_l = -0.5 * torch.sum(1. + logvar - mu.pow(2) - logvar.exp()) / x.shape[0] / 1e4\n",
    "        # kld_o = -1. * F.kl_div(xhat, x, reduction='mean')\n",
    "        loss = mse + self._beta_scheduler(ep) * kld_l  # + 1 * kld_o\n",
    "\n",
    "        if train:\n",
    "            self.train_loss['BCE'].append(bce.item())\n",
    "            self.train_loss['MSE'].append(mse.item())\n",
    "            self.train_loss['KL_latent'].append(kld_l.item())\n",
    "            self.train_loss['Loss'].append(loss.item())\n",
    "        else:\n",
    "            self.test_loss['BCE'].append(bce.item())\n",
    "            self.test_loss['MSE'].append(mse.item())\n",
    "            self.test_loss['KL_latent'].append(kld_l.item())\n",
    "            self.test_loss['Loss'].append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def _report_train(self, i):\n",
    "        \"\"\"Report training metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        if (i % self.print_every == 0):\n",
    "            print(\"Training iteration %i, global step %i\" % (i + 1, self.num_steps))\n",
    "            print(\"BCE : %3.4f\" % (self.train_loss['BCE'][-1]))\n",
    "            print(\"MSE : %3.4f\" % (self.train_loss['MSE'][-1]))\n",
    "            print(\"KL_l: %3.4f\" % (self.train_loss['KL_latent'][-1]))\n",
    "            print(\"Loss: %3.4f\" % (self.train_loss['Loss'][-1]))\n",
    "            print(\"__\"*20)\n",
    "\n",
    "    def _report_test(self, ep):\n",
    "        \"\"\"Report testing metrics to logger and standard output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            training step\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "        print('*** TEST LOSS ***')\n",
    "        print(\"Epoch %i, global step %i\" % (ep, self.num_steps))\n",
    "        print(\"BCE : %3.4f\" % (self.test_loss['BCE'][-1]))\n",
    "        print(\"MSE : %3.4f\" % (self.test_loss['MSE'][-1]))\n",
    "        print(\"KL_l: %3.4f\" % (self.test_loss['KL_latent'][-1]))\n",
    "        print(\"Loss: %3.4f\" % (self.test_loss['Loss'][-1]))\n",
    "\n",
    "        print(\"__\"*20)\n",
    "\n",
    "    def _beta_scheduler(self, epoch, beta0=0., step=50, gamma=0.1):\n",
    "        \"\"\"Scheduler for beta value, the sheduler is a step function that\n",
    "        increases beta value after \"step\" number of epochs by a factor \"gamma\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch : int\n",
    "            epoch value\n",
    "        beta0 : float\n",
    "            starting beta value\n",
    "        step  : int\n",
    "            epoch step for update\n",
    "        gamma : float\n",
    "            linear factor of step scheduler\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta\n",
    "            beta value\n",
    "        \"\"\"\n",
    "\n",
    "        if self.beta == 'step':\n",
    "            return beta0 + gamma * (epoch // step)\n",
    "        else:\n",
    "            return float(self.beta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:27:30.868975881Z",
     "start_time": "2023-12-14T15:27:30.849417677Z"
    }
   },
   "id": "81fd6eb67395a1ac"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is model in cuda?  True\n"
     ]
    }
   ],
   "source": [
    "# initialize trainer \n",
    "trainer = Trainer(model=model, optimizer=optimizer, batch_size=32, \n",
    "                  print_every=100, scheduler=scheduler,\n",
    "                  device=device)\n",
    "\n",
    "# create data loaders (feed data to model for each fold)\n",
    "train_loader, test_loader = dataset.get_dataloader(batch_size=32, test_split=.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:29:14.570374916Z",
     "start_time": "2023-12-14T15:29:14.527750547Z"
    }
   },
   "id": "c7fcec91f1f39d9a"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "\n",
      "Epoch 1\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 1\n",
      "BCE : 0.6301\n",
      "MSE : 0.0022\n",
      "KL_l: 0.0043\n",
      "Loss: 0.0022\n",
      "________________________________________\n",
      "Training iteration 101, global step 101\n",
      "BCE : 0.6477\n",
      "MSE : 0.0016\n",
      "KL_l: 0.0044\n",
      "Loss: 0.0016\n",
      "________________________________________\n",
      "Training iteration 201, global step 201\n",
      "BCE : 0.6521\n",
      "MSE : 0.0020\n",
      "KL_l: 0.0045\n",
      "Loss: 0.0020\n",
      "________________________________________\n",
      "Training iteration 301, global step 301\n",
      "BCE : 0.6308\n",
      "MSE : 0.0023\n",
      "KL_l: 0.0047\n",
      "Loss: 0.0023\n",
      "________________________________________\n",
      "Training iteration 401, global step 401\n",
      "BCE : 0.6362\n",
      "MSE : 0.0016\n",
      "KL_l: 0.0048\n",
      "Loss: 0.0016\n",
      "________________________________________\n",
      "Training iteration 501, global step 501\n",
      "BCE : 0.6378\n",
      "MSE : 0.0018\n",
      "KL_l: 0.0048\n",
      "Loss: 0.0018\n",
      "________________________________________\n",
      "Training iteration 601, global step 601\n",
      "BCE : 0.6332\n",
      "MSE : 0.0018\n",
      "KL_l: 0.0050\n",
      "Loss: 0.0018\n",
      "________________________________________\n",
      "Training iteration 701, global step 701\n",
      "BCE : 0.6139\n",
      "MSE : 0.0019\n",
      "KL_l: 0.0051\n",
      "Loss: 0.0019\n",
      "________________________________________\n",
      "Training iteration 801, global step 801\n",
      "BCE : 0.6447\n",
      "MSE : 0.0014\n",
      "KL_l: 0.0050\n",
      "Loss: 0.0014\n",
      "________________________________________\n",
      "Training iteration 901, global step 901\n",
      "BCE : 0.6360\n",
      "MSE : 0.0015\n",
      "KL_l: 0.0051\n",
      "Loss: 0.0015\n",
      "________________________________________\n",
      "Training iteration 1001, global step 1001\n",
      "BCE : 0.6317\n",
      "MSE : 0.0011\n",
      "KL_l: 0.0052\n",
      "Loss: 0.0011\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 1, global step 1080\n",
      "BCE : 0.6255\n",
      "MSE : 0.0011\n",
      "KL_l: 0.0053\n",
      "Loss: 0.0011\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.08 m\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "Epoch 2\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 1081\n",
      "BCE : 0.6421\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0052\n",
      "Loss: 0.0010\n",
      "________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 101, global step 1181\n",
      "BCE : 0.6494\n",
      "MSE : 0.0010\n",
      "KL_l: 0.0054\n",
      "Loss: 0.0010\n",
      "________________________________________\n",
      "Training iteration 201, global step 1281\n",
      "BCE : 0.6328\n",
      "MSE : 0.0011\n",
      "KL_l: 0.0054\n",
      "Loss: 0.0011\n",
      "________________________________________\n",
      "Training iteration 301, global step 1381\n",
      "BCE : 0.6401\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0055\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 401, global step 1481\n",
      "BCE : 0.6533\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0056\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 501, global step 1581\n",
      "BCE : 0.6256\n",
      "MSE : 0.0006\n",
      "KL_l: 0.0057\n",
      "Loss: 0.0006\n",
      "________________________________________\n",
      "Training iteration 601, global step 1681\n",
      "BCE : 0.6084\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0057\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 701, global step 1781\n",
      "BCE : 0.6279\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0057\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 801, global step 1881\n",
      "BCE : 0.6478\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0058\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 901, global step 1981\n",
      "BCE : 0.6386\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0059\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 1001, global step 2081\n",
      "BCE : 0.6394\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0059\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 2, global step 2160\n",
      "BCE : 0.6231\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0059\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.17 m\n",
      "########################################\n",
      "EarlyStopping counter: 1 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 3\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 2161\n",
      "BCE : 0.6381\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0060\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 101, global step 2261\n",
      "BCE : 0.6185\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0060\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 201, global step 2361\n",
      "BCE : 0.6497\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0061\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 301, global step 2461\n",
      "BCE : 0.6218\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0061\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 401, global step 2561\n",
      "BCE : 0.6265\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0061\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 501, global step 2661\n",
      "BCE : 0.6249\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0062\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 601, global step 2761\n",
      "BCE : 0.6340\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0063\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 701, global step 2861\n",
      "BCE : 0.6313\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0064\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 801, global step 2961\n",
      "BCE : 0.6184\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0064\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 901, global step 3061\n",
      "BCE : 0.6301\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0064\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 1001, global step 3161\n",
      "BCE : 0.6192\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0065\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 3, global step 3240\n",
      "BCE : 0.6229\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0065\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.25 m\n",
      "########################################\n",
      "EarlyStopping counter: 2 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 4\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 3241\n",
      "BCE : 0.6289\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0065\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 101, global step 3341\n",
      "BCE : 0.6170\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0066\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 201, global step 3441\n",
      "BCE : 0.6204\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0067\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 301, global step 3541\n",
      "BCE : 0.6268\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0067\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 401, global step 3641\n",
      "BCE : 0.6402\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0068\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 501, global step 3741\n",
      "BCE : 0.6138\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0068\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Training iteration 601, global step 3841\n",
      "BCE : 0.6271\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0068\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 701, global step 3941\n",
      "BCE : 0.6213\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0069\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 801, global step 4041\n",
      "BCE : 0.6356\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0070\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 901, global step 4141\n",
      "BCE : 0.6405\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0070\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 1001, global step 4241\n",
      "BCE : 0.6351\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0070\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 4, global step 4320\n",
      "BCE : 0.6261\n",
      "MSE : 0.0004\n",
      "KL_l: 0.0071\n",
      "Loss: 0.0004\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.33 m\n",
      "########################################\n",
      "EarlyStopping counter: 3 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 5\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 4321\n",
      "BCE : 0.6260\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0071\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 101, global step 4421\n",
      "BCE : 0.6466\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0072\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 201, global step 4521\n",
      "BCE : 0.6246\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0072\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 301, global step 4621\n",
      "BCE : 0.6436\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0073\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 401, global step 4721\n",
      "BCE : 0.6232\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0072\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 501, global step 4821\n",
      "BCE : 0.6116\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0073\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 601, global step 4921\n",
      "BCE : 0.6546\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0075\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 701, global step 5021\n",
      "BCE : 0.6358\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0075\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 801, global step 5121\n",
      "BCE : 0.6091\n",
      "MSE : 0.0005\n",
      "KL_l: 0.0074\n",
      "Loss: 0.0005\n",
      "________________________________________\n",
      "Training iteration 901, global step 5221\n",
      "BCE : 0.6111\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0075\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 1001, global step 5321\n",
      "BCE : 0.6279\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0075\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 5, global step 5400\n",
      "BCE : 0.6229\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0075\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.40 m\n",
      "########################################\n",
      "EarlyStopping counter: 4 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 6\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 5401\n",
      "BCE : 0.6317\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0076\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 101, global step 5501\n",
      "BCE : 0.6279\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0076\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 201, global step 5601\n",
      "BCE : 0.6292\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0076\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 301, global step 5701\n",
      "BCE : 0.6511\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0077\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 401, global step 5801\n",
      "BCE : 0.6259\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0076\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 501, global step 5901\n",
      "BCE : 0.6125\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0076\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 601, global step 6001\n",
      "BCE : 0.6373\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0077\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 701, global step 6101\n",
      "BCE : 0.6105\n",
      "MSE : 0.0003\n",
      "KL_l: 0.0077\n",
      "Loss: 0.0003\n",
      "________________________________________\n",
      "Training iteration 801, global step 6201\n",
      "BCE : 0.6241\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0078\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 901, global step 6301\n",
      "BCE : 0.6393\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 6401\n",
      "BCE : 0.6353\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 6, global step 6480\n",
      "BCE : 0.6274\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.48 m\n",
      "########################################\n",
      "EarlyStopping counter: 5 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 7\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 6481\n",
      "BCE : 0.6433\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 101, global step 6581\n",
      "BCE : 0.6191\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0078\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 201, global step 6681\n",
      "BCE : 0.6234\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 301, global step 6781\n",
      "BCE : 0.6100\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0078\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 401, global step 6881\n",
      "BCE : 0.6455\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0081\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 501, global step 6981\n",
      "BCE : 0.6245\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 601, global step 7081\n",
      "BCE : 0.6183\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0079\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 701, global step 7181\n",
      "BCE : 0.6255\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0080\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 801, global step 7281\n",
      "BCE : 0.6459\n",
      "MSE : 0.0002\n",
      "KL_l: 0.0081\n",
      "Loss: 0.0002\n",
      "________________________________________\n",
      "Training iteration 901, global step 7381\n",
      "BCE : 0.6201\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0080\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 7481\n",
      "BCE : 0.6198\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0080\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 7, global step 7560\n",
      "BCE : 0.6356\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0081\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.57 m\n",
      "########################################\n",
      "EarlyStopping counter: 6 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 8\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 7561\n",
      "BCE : 0.6168\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0080\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 101, global step 7661\n",
      "BCE : 0.6299\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 201, global step 7761\n",
      "BCE : 0.6236\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0081\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 301, global step 7861\n",
      "BCE : 0.6318\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 401, global step 7961\n",
      "BCE : 0.6271\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 501, global step 8061\n",
      "BCE : 0.6293\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 601, global step 8161\n",
      "BCE : 0.6379\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 701, global step 8261\n",
      "BCE : 0.6181\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 801, global step 8361\n",
      "BCE : 0.6429\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0084\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 901, global step 8461\n",
      "BCE : 0.6365\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 8561\n",
      "BCE : 0.6180\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0082\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 8, global step 8640\n",
      "BCE : 0.6356\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Time per epoch:  5  s\n",
      "Elapsed time  : 0.65 m\n",
      "########################################\n",
      "EarlyStopping counter: 7 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 9\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 8641\n",
      "BCE : 0.6276\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 101, global step 8741\n",
      "BCE : 0.6101\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 201, global step 8841\n",
      "BCE : 0.6450\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0085\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 301, global step 8941\n",
      "BCE : 0.6259\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0084\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 401, global step 9041\n",
      "BCE : 0.6143\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0083\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 501, global step 9141\n",
      "BCE : 0.6396\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0085\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 601, global step 9241\n",
      "BCE : 0.6266\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0085\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 701, global step 9341\n",
      "BCE : 0.6318\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0085\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 801, global step 9441\n",
      "BCE : 0.6356\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 901, global step 9541\n",
      "BCE : 0.6182\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 9641\n",
      "BCE : 0.6146\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 9, global step 9720\n",
      "BCE : 0.6329\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.73 m\n",
      "########################################\n",
      "EarlyStopping counter: 8 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 10\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 9721\n",
      "BCE : 0.6433\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 101, global step 9821\n",
      "BCE : 0.6254\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 201, global step 9921\n",
      "BCE : 0.6194\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0087\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 301, global step 10021\n",
      "BCE : 0.6420\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0089\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 401, global step 10121\n",
      "BCE : 0.6333\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0089\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 501, global step 10221\n",
      "BCE : 0.6367\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0089\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 601, global step 10321\n",
      "BCE : 0.6383\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0089\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 701, global step 10421\n",
      "BCE : 0.6311\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0090\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 801, global step 10521\n",
      "BCE : 0.6299\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0090\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 901, global step 10621\n",
      "BCE : 0.6260\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0089\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 10721\n",
      "BCE : 0.6382\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0092\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 10, global step 10800\n",
      "BCE : 0.6431\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0091\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.82 m\n",
      "########################################\n",
      "EarlyStopping counter: 9 / 10\n",
      "########################################\n",
      "\n",
      "Epoch 11\n",
      "beta: 0.00\n",
      "Training iteration 1, global step 10801\n",
      "BCE : 0.6303\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0091\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 101, global step 10901\n",
      "BCE : 0.6314\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0092\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 201, global step 11001\n",
      "BCE : 0.6205\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0091\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 301, global step 11101\n",
      "BCE : 0.6297\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0092\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 401, global step 11201\n",
      "BCE : 0.6367\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0092\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 501, global step 11301\n",
      "BCE : 0.6053\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0091\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 601, global step 11401\n",
      "BCE : 0.6434\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0093\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 701, global step 11501\n",
      "BCE : 0.6234\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0092\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 801, global step 11601\n",
      "BCE : 0.6370\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0094\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Training iteration 901, global step 11701\n",
      "BCE : 0.6299\n",
      "MSE : 0.0001\n",
      "KL_l: 0.0094\n",
      "Loss: 0.0001\n",
      "________________________________________\n",
      "Training iteration 1001, global step 11801\n",
      "BCE : 0.6370\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0094\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "*** TEST LOSS ***\n",
      "Epoch 11, global step 11880\n",
      "BCE : 0.6229\n",
      "MSE : 0.0000\n",
      "KL_l: 0.0093\n",
      "Loss: 0.0000\n",
      "________________________________________\n",
      "Time per epoch:  4  s\n",
      "Elapsed time  : 0.90 m\n",
      "########################################\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, test_loader, epochs=400, save=True, early_stop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:30:08.929166995Z",
     "start_time": "2023-12-14T15:29:14.819526761Z"
    }
   },
   "id": "ad1bdfcbb1b0f37f"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Explore model\n",
    "X_train = \n",
    "with torch.no_grad():\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T15:27:03.338219732Z",
     "start_time": "2023-12-14T15:27:03.334923618Z"
    }
   },
   "id": "3abd0d9d3f64b87f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21e165c310138070"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
